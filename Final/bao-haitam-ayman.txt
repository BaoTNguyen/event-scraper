---
title: "Event Scraper for In-Person Networking"
author: "Bao Nguyen, Haitam Aksikas, Ayman Arman"
format: html
editor: visual
---

```{r}
library(purrr) 
library(tibble) 
library(arrow) 
library(tidyverse) 
library(spacyr) 
library(textdata) 
library(jsonlite) 
library(tidytext)
library(ggraph)
library(igraph)
library(stringi)
library(glue)
```

## *Part 1: Scraping the data from 3 event websites (EdmontonRIN, PlatformCalgary, and EventBrite).*

```{r}
edmontonrin <- system("node ./edmontonrin.js", intern = TRUE) %>%
  fromJSON() %>%
  as_tibble()
```

```{r}
platformcalgary <- system("node ./platformcalgary.js", intern = TRUE) %>%
  fromJSON() %>% 
  as_tibble()
```

```{r}
eventbrite <- system("node ./eventbrite.js", intern = TRUE) %>%
  fromJSON() %>% 
  as_tibble()
```

***Putting all the information together in 1 formatted table (all_events)***

```{r}
# all_events_raw <- bind_rows(edmontonrin, platformcalgary, eventbrite) %>% 
all_events_raw <- bind_rows(edmontonrin, platformcalgary) %>% 
  dplyr::mutate(event_id = row_number()) %>%
  dplyr::select(event_id, dplyr::everything())

```

```{r}
# Preprocess description for spaCy
all_events <- all_events_raw %>%
  mutate(
    description_clean = description %>%
      str_replace_all("([a-z])(\\.[A-Z])", "\\1. \\2") %>%   # add space after a period glued to a capital
      str_replace_all("([a-z])(,[A-Z])", "\\1, \\2") %>%     # add space after a comma glued to a capital
      str_replace_all("([A-Za-z])'s", "\\1 's") %>%          # ensure space before possessive ’s
      str_replace_all("([A-Z]{2,})([A-Z][a-z])", "\\1 \\2") %>%  # split run-on capitals
      str_squish()
  )

```


```{r}
# Install and prepare spaCy transformer model
spacy_install()
spacy_download_langmodel("en_core_web_trf")
```


```{r}
# Initialize spaCy
spacy_initialize(model = "en_core_web_trf")

# NER extraction across all descriptions
ner_df <- spacy_extract_entity(
  all_events$description,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% 
  tibble::as_tibble() %>% # Preprocessing dataframe before further cleaning
  filter(!str_detect(text, "@"), # Remove email addresses
         !str_detect(text, "https?://")) # Remove links

# Noun phrase extraction across all descriptions
np_df <- spacy_extract_nounphrases(
  all_events$description,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% 
  tibble::as_tibble()

spacy_finalize()

# Keep useful entity types and regroup by category
keep_types <- c("PERSON", "ORG", "GPE")
entity_category_map <- tibble(
  entity_type = keep_types,
  category = c("people", "organizations", "places")
)

# Remove stop words and prepare clean entities for each event
process_entities <- function(ner_df) {
  ner_df %>%
    filter(ent_type %in% keep_types) %>%
    mutate(
      entity_id   = row_number(), # Label each event
      orig_entity = text # Save original text
    ) %>%
    unnest_tokens(token, text) %>%
    anti_join(stop_words, by = c("token" = "word")) %>%
    group_by(doc_id, ent_type, entity_id, orig_entity) %>%
    summarise(
      entity_clean_tokens = list(token), # Get list of clean entities for each event
      .groups = "drop"
    ) %>%
    mutate(
      entity_clean = map_chr(entity_clean_tokens, ~ str_c(.x, collapse = " ")),
      entity_clean = str_squish(entity_clean),
      entity_clean = if_else(
        str_detect(orig_entity, "[A-Z]{2,}") | str_detect(orig_entity, "[A-Za-z][A-Z]"),
        orig_entity, # keep acronyms/mixed case
        str_to_title(entity_clean) # normalize regular names
      )
    ) %>%
    filter(entity_clean != "") %>%
    distinct(doc_id, ent_type, entity_clean) %>%
    left_join(entity_category_map, by = c("ent_type" = "entity_type")) %>%
    group_by(doc_id, category) %>%
    summarise(
      entities = list(unique(entity_clean)), # deduplicated vectors
      .groups = "drop"
    ) %>% 
    pivot_wider(names_from = category, values_from = entities) %>% 
    mutate(across(c(people, organizations, places),
                  ~ map(.x, ~ if (length(.x) == 0) NA_character_ else .x)))
}

# Call function
entities_tbl <- process_entities(ner_df)

# Remove stop words from extracted noun phrases
process_nounphrases <- function(np_df) {
  np_df %>% 
    unnest_tokens(output = word, input = text) %>% 
    anti_join(stop_words, by = "word") %>%
    group_by(doc_id, root_id, start_id, length) %>%  # keys for a phrase
    summarise(clean_text = str_c(word, collapse = " "),
              length = n(),
              .groups = "drop") %>%
    filter(clean_text != "") %>% 
    filter(length > 1) %>%
    mutate(clean_text = str_squish(clean_text)) %>%
    group_by(doc_id) %>%
    summarise(noun_phrases = list(unique(clean_text)), .groups = "drop")
}

# Call function
nounphrases_tbl <- process_nounphrases(np_df)

# Attach enriched info to combined events for downstream personas / visualizations
all_events_enriched <- all_events %>%
  left_join(entities_tbl, join_by("event_id" == "doc_id")) %>%
  left_join(nounphrases_tbl, join_by("event_id" == "doc_id"))

all_events_enriched
```

## *Part 2: NLP Analysis (Ayman)*

### *Why are we using NLP?*

-   *Generate a precise understanding of the topic of the event.*

-   *It is possible that the most used words are not the main topic of the event.*

-   *We may find a relevant event, but still may want to understand the central theme.*

-   *We can create criteria by which we can choose top 3 events.*

    -   *We can categorize events (i.e. "networking", "seminar", "conference", etc.)*

    -   *Is it free or paid?*

    -   *Likelihood of professionals attending?*

    -   *Is it local? regional? national?*

    -   *Is the event open to all people/professions?*

    -   *And much much more potentially.*

-   *Understand the sentiment of the event. Is it positive or negative? Emphasis on Opportunity or Danger/Risks?*

### *How can we use NLP?*

-   *We can track how the DoW impacts what sort of event is taking place (12.4.6 Story Through Time)*

-   *Using correlation mapping, we can explore how different themes connect to one another, providing graphs to the end user exploring how their key words are explored in the context of the event.*

-   *We can create a lexicon for words of interest and assign a sentiment score to each event (based on whichever criteria we deem useful)*

------------------------------------------------------------------------

```{r}
nlp_tokens <- all_events %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description, token = "ngrams", n = 3)

nlp_token_1 <- all_events %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description)%>%
  dplyr::anti_join(tidytext::stop_words)


nlp_tokens <- nlp_tokens %>% 
  tidyr::separate_wider_delim(word, delim = " ", names = c("item1",
                                                           "item2",
                                                           "item3"
                                                           ))

# for loop to remove stop_words
for(i in 2:nrow(nlp_tokens)){
  for(j in 2:length(nlp_tokens)){
    
    if(nlp_tokens[i, j] %in% tidytext::stop_words$word){
      
      nlp_tokens[i,j] <- ""
      
    }else{
      nlp_tokens[i,j] <- nlp_tokens[i,j]
    }
  }
}


nlp_tokens <- nlp_tokens %>% 
  tidyr::unite(item1, 2:length(nlp_tokens),  sep = " ")%>%
  dplyr::transmute(event_id, phrase = item1)

nlp_tokens$phrase <- nlp_tokens$phrase %>% gsub("^\\s+|\\s+$", "", .)

nlp_tokens <- nlp_tokens %>% dplyr::filter(stri_count_words(phrase) >= 3) %>% unique()
```

*Evaluate the entire above chunk for nlp_tokens, which will be the base for further analysis*

### *Lexicon(s) Creation \*\* Requires Updating*

```{r}
networking_words <- list(
  
  keywords = c("network","networking", "meet", "job", "work","friend",
               "relationship", "mixer","peer"),
  
  others = c("engage", "exchange","social", "mingle", "introduction", 
             "collaborate", "partnership", "discuss", "roundtable")
  
)

```

```{r}
persona_finance_student <- list(
  
  keywords = c(
    "finance", "financial", "investment", "investing", "markets",
    "economics", "trading", "derivatives", "valuation", "analytics",
    "data", "economy", "portfolio", "risk", "venture", "credit", "debt",
    "banking", "capital", "wealth", "equity", "fund", "money"
  ),
  
  priority_keywords = c("finance", "internship", "trading", "analyst", "mentorship", "professional"),
  
  locations = c("virtual", "online"),
  
  event_types = c("networking", "career fair", "seminar")
)
```

```{r}
persona_tech_student <- list(
  
  keywords = c(
    "tech", "technology", "software", "developer", "coding",
    "programming", "data", "python", "r", "cloud", "app",
    "machine", "learning", "ai", "intelligence", "artificial"
  ),
  
  priority_keywords = c(
    "hackathon", "developer", "internship", "programming"
  ),
  
  locations = c("calgary"),
  
  event_types = c("workshop", "webinar", "hackathon")
)

```

```{r}
persona_entrepreneur <- list(
  
  keywords = c(
    "entrepreneur", "startup", "business", "innovation",
    "pitch", "founder", "funding", "capital", "growth", "strategy",
    "marketing", "strategy", "leadership", "sales", "money", "sales"
  ),
  
  priority_keywords = c(
    "sales", "pitch", "startup", "entrepreneur", "mentor"
  ),
  
  locations = c("virtual", "online"),
  
  event_types = c(
    "networking", "pitch", "seminar"
  )
)
```


### *NEW IDEAS:*

1.  *I want to know which key words are being used in which events. I can potentially make a faceted graph showing keyword use in descriptions. –\> this lends some weight to understanding the specific topic of each event.*

### [*Outputs:*]{.underline}

### *Bar chart where x axis is count of persona matches and y axis is event id.*

### *nlp_fit, a tibble with event_id, non-stop words, and \# of mentions in description*

```{r}


topic <- persona_finance_student$keywords # choose your topic of interest here

nlp_token_1$Contains <- "" # initializing the contains column 


for(i in 1:nrow(nlp_token_1)){

nlp_token_1$Contains[i] <- grepl(nlp_token_1[i,2], paste(topic, collapse = ""))

}

nlp_fit <- nlp_token_1 %>%
  dplyr::filter(Contains == "FALSE")%>%
  dplyr::group_by(event_id)%>%
  dplyr::count(word, name = "Matches")%>%
  dplyr::arrange(desc(Matches))

nlp_fit %>%
  dplyr::group_by(event_id)%>% dplyr::summarise(Matches = sum(Matches))%>%
  dplyr::arrange(desc(Matches))%>% dplyr::slice_head(n = 10)%>%
  ggplot2::ggplot(aes(x = Matches, y = reorder(event_id, Matches), fill = Matches))+
  geom_col()+
  theme(panel.background = element_blank(), legend.position = "none")+
  labs(x = "Matches", y = "Event ID")




```

```{r}

# Attempting same logic as matches first, **no use for lexicon dataframe
#Initializing dataframe (this is not AI btw)

nlp_type <- nlp_token_1
nlp_type$Networking <- ""



for(i in 1:nrow(nlp_type)){

nlp_type$Networking[i] <- grepl(nlp_type[i,2], paste(networking_words, collapse = ""))


}

nlp_type <- nlp_type %>%
  dplyr::mutate(across(3:ncol(nlp_type), ~ as.integer(as.logical(.))))


nlp_type %>%
  tidyr::pivot_longer(cols = c(4:ncol(nlp_type)),
                      names_to = "Type",
                      values_to = "Score")%>%
  dplyr::filter(Score != 0)%>%
  ggplot2::ggplot(aes(x = as.character(event_id), 
                      y = Score,
                      fill = Score))+
  geom_col()+
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.background = element_rect(fill = "lightgrey"),
        legend.position = "none")+
  labs(title = "Event Type Scores",
       )+
  coord_flip()
  


# Not generalized so far
nlp_type_scores <- nlp_type %>%
  dplyr::group_by(event_id)%>%
  dplyr::summarise(Network_Score = sum(Networking),
                   Matches = sum(Contains))%>%
  dplyr::filter(Matches == 1)

```

```{r}


event_words <- nlp_type %>% dplyr::filter(Contains != 0)%>%
  dplyr::select(event_id)%>% left_join(., nlp_token_1, by = "event_id")%>%dplyr::group_by(event_id)%>%
  dplyr::count(word, name = "Instances")%>%
  dplyr::arrange(desc(Instances))


colors <- rainbow(17, alpha = .5)

colors2 <- apply(event_words[,2:3], 1, FUN = function(x) colors)



event_igraph <- event_words%>%  dplyr::group_by(word)%>%
  igraph::graph_from_data_frame()


event_igraph%>%
  ggraph(layout = "nicely")+
  geom_edge_link(show.legend = FALSE)+
  geom_node_point(colour = "orange", size = 5)+
  geom_node_text(aes(label = name),
                 size = 4,
                 vjust = 1.3,
                 hjust = 0.9)+
  theme_void()


```

### Machine Learning Workflow

```{r}
desc_tokens <- all_events %>%
  select(event_id, description) %>%
  filter(!is.na(description)) %>%
  tidytext::unnest_tokens(output = word, input = description) %>%
  anti_join(tidytext::stop_words)

title_tokens <- all_events %>%
  select(event_id, title) %>%
  filter(!is.na(title)) %>%
  tidytext::unnest_tokens(output = word, input = title) %>%
  anti_join(tidytext::stop_words, by = "word")

```


```{r}
# DONE MATCH DESC SYNC WITH NLP
match_desc <- function(tokens, persona) {

  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }
# counting each instance per event that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Desc_Matches")
  
}

#__________________________________________________________________________________
# DONE MATCH TITLE SYNC WITH NLP
match_title <- function(tokens, persona) {
  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }

# counting each instance per event title that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Title_Matches")
}

#__________________________________________________________________________________

event_sentiment_key <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$keywords, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Key_Matches")


}

#__________________________________________________________________________________

event_sentiment_other <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$others, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Other_Matches")

}

#__________________________________________________________________________________


match_location <- function(events, persona) {
  events %>%
    mutate(
      location_lower = tolower(location),
      Location_Matches = if_else(
        str_detect(location_lower, str_c(persona$locations, collapse = "|")),
        1, 0, missing = 0)) %>%
    select(event_id, Location_Matches)
}


match_type <- function(events, persona) {
  events %>%
    mutate(
      desc_lower = tolower(description),
      Type_Matches = if_else(
        str_detect(desc_lower, str_c(persona$event_types, collapse = "|")),
        1L, 0L, missing = 0L)) %>%
    select(event_id, Type_Matches)
}

#__________________________________________________________________________________

match_highly_relevant <- function(){
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
}






```

```{r}
################### STILL TESTING (DON'T IMPLEMENT YET) #####################
# Function that builds metrics for each persona
make_persona_features <- function(events, desc_tokens, title_tokens, persona, suffix) {
  desc_hits <- desc_tokens %>%
    filter(word %in% persona$keywords) %>%
    count(event_id, name = glue::glue("Desc_Matches_{suffix}"))
  
  priority_hits <- desc_tokens %>%
    filter(word %in% persona$priority_keywords) %>%
    count(event_id, name = glue::glue("Desc_Priority_Matches_{suffix}"))
  
  title_hits <- title_tokens %>%
    filter(word %in% persona$keywords) %>%
    count(event_id, name = glue::glue("Title_Matches_{suffix}"))
  
  loc_hits <- events %>%
    mutate(Location_Match = if_else(
      str_detect(tolower(location), str_c(persona$locations, collapse = "|")),
      1, 0, missing = 0)) %>%
    select(event_id, Location_Match)
  
  type_hits <- events %>%
    mutate(Type_Match = if_else(
      str_detect(tolower(description), str_c(persona$event_types, collapse = "|")),
      1, 0, missing = 0)) %>%
    select(event_id, Type_Match)
  
  events %>%
    select(event_id) %>%
    left_join(desc_hits,     by = "event_id") %>%
    left_join(priority_hits, by = "event_id") %>%
    left_join(title_hits,    by = "event_id") %>%
    left_join(loc_hits,      by = "event_id") %>%
    left_join(type_hits,     by = "event_id") %>%
    mutate(across(-event_id, \(x) replace_na(x, 0))) %>%
    rename_with(~ glue::glue("{.}_{suffix}"), -event_id)
}

# Tokenize once
desc_tokens <- all_events_enriched %>%
  select(event_id, description) %>%
  filter(!is.na(description)) %>%
  unnest_tokens(word, description) %>%
  anti_join(stop_words, by = "word")

title_tokens <- all_events_enriched %>%
  select(event_id, title) %>%
  filter(!is.na(title)) %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words, by = "word")

# Build personas list
persona_list <- list(
  fin  = persona_finance_student,
  tech = persona_tech_student,
  ent  = persona_entrepreneur
)

# combine persona-level features
persona_features <- map2(
  persona_list,
  names(persona_list),
  ~ make_persona_features(
      events       = all_events_enriched,
      desc_tokens  = desc_tokens,
      title_tokens = title_tokens,
      persona      = .x,
      suffix       = .y
    )
)

events_features <- reduce(persona_features, left_join, by = "event_id")

# Calculate coverage score
# assume persona_list is named: list(fin = ..., tech = ..., ent = ...)
persona_names <- names(persona_list)
num_personas  <- length(persona_list)

# -------- Normalized Persona Coverage (0–4 points) --------
coverage_cols <- map(persona_names, \(nm) {
  keywords_n <- length(persona_list[[nm]]$keywords)
  desc_col   <- glue::glue("Desc_Matches_{nm}")
  title_col  <- glue::glue("Title_Matches_{nm}")
  matches    <- events_features[[desc_col]] + events_features[[title_col]]
  if (keywords_n == 0) return(rep(0, nrow(events_features)))
  pmin(1, matches / keywords_n)
}) %>%
  set_names(persona_names) %>%
  as_tibble()

coverage_score <- if (ncol(coverage_cols) == 0) {
  rep(0, nrow(events_features))
} else {
  rowMeans(coverage_cols, na.rm = TRUE)
}
coverage_score <- coverage_score * 4

# -------- Persona Score Envelope (0–4 points) --------
envelope_cols <- map(persona_names, \(nm) {
  priority_col <- glue::glue("Desc_Priority_Matches_{nm}")
  general_col  <- glue::glue("Desc_Matches_{nm}")
  loc_col      <- glue::glue("Location_Match_{nm}")
  type_col     <- glue::glue("Type_Match_{nm}")
  
  priority_norm <- pmin(1, events_features[[priority_col]] / 2)
  general_norm  <- pmin(1, events_features[[general_col]] / 5)
  loc_bonus     <- events_features[[loc_col]]
  type_bonus    <- events_features[[type_col]]
  
  0.6 * priority_norm + 0.3 * general_norm + 0.05 * loc_bonus + 0.05 * type_bonus
})

envelope_score <- if (length(envelope_cols) == 0) {
  rep(0, nrow(events_features))
} else {
  do.call(pmax, c(envelope_cols, na.rm = TRUE))
}
envelope_score <- envelope_score * 4


# -------- Persona Diversity Index (0–2 points) --------
diversity_flags <- map(persona_names, \(nm) {
  priority_col <- glue::glue("Desc_Priority_Matches_{nm}")
  general_col  <- glue::glue("Desc_Matches_{nm}")
  title_col    <- glue::glue("Title_Matches_{nm}")
  
  priority_flag <- events_features[[priority_col]] >= 1
  regular_flag  <- (events_features[[general_col]] + events_features[[title_col]]) >= 2
  
  as.integer(priority_flag & regular_flag)
})

diversity_count <- reduce(diversity_flags, `+`)
diversity_score <- 2 * (diversity_count / num_personas)

# -------- Final relevance score (0–10) --------
events_relevance <- events_features %>%
  mutate(
    coverage_score  = coverage_score,
    envelope_score  = envelope_score,
    diversity_score = diversity_score,
    relevance_score = pmin(10, round(coverage_score + envelope_score + diversity_score, 2))
  ) %>%
  select(event_id, coverage_score, envelope_score, diversity_score, relevance_score)

events_relevance


```

```{r}
score_event_relevance <- function(events_features) {
  
  scored <- events_features %>%
    mutate(
      raw_score =
        1.0 * Desc_Matches +
        1.5 * Title_Matches +
        1.5 * Sentiment_Key_Matches +
        0.2 * Sentiment_Other_Matches +
        1.0 * Location_Matches +
        1.0 * Type_Matches
    )
  
  min_val <- min(scored$raw_score, na.rm = TRUE)
  max_val <- max(scored$raw_score, na.rm = TRUE)
  
  scored %>%
    mutate(relevance_score = round(1 + 9 * ((raw_score - min_val) / (max_val - min_val)), 2)) %>%
    
    arrange(desc(relevance_score))
}

```

```{r}
get_top_events <- function(events_scored, n = 3) {
  events_scored %>%
    slice_head(n = n) %>%
    select(
      title,
      date,
      start_time,
      end_time,
      location,
      description,
      event_url,
      relevance_score
    )
}
```


```{r}
events_features_1 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_finance_student,
  networking_words
)

events_scored_1 <- score_event_relevance(events_features_1)
top3_events_1 <- get_top_events(events_scored_1, 3)
top3_events_1

```



```{r}
events_features_2 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_tech_student,
  networking_words
)

events_scored_2 <- score_event_relevance(events_features_2)
top3_events_2 <- get_top_events(events_scored_2, 3)
top3_events_2
```



```{r}
events_features_3 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_entrepreneur,
  networking_words
)

events_scored_3 <- score_event_relevance(events_features_3)
top3_events_3 <- get_top_events(events_scored_3, 3)
top3_events_3
```



What Next: 

Rough Structure of Report. Section where each visualization will belong.



Report:

- Intro: Framing the Problem/hypothesis
  - Problems with the job search, and proposal 


































