---
title: "Event Scraper for In-Person Networking"
author: "Bao Nguyen, Haitam Aksikas, Ayman Arman"
format: html
editor: visual
---

{r, include = FALSE}

knitr::opts_chunk$set(echo = FALSE,
                  warning = FALSE,
                  message = FALSE)


{r}
library(purrr) 
library(tibble) 
library(tidyverse) 
library(spacyr) 
library(textdata) 
library(jsonlite) 
library(tidytext)
library(igraph)
library(ggraph)
library(gt)
library(stringi) 

1. Introduction

1.1. The Current Job Market

Job seekers are focusing more on securing internships or new graduate roles through strong connections and sponsors within their field. Young professionals are increasingly reliant on open invitation networking events to connect with individuals and begin forming a robust network around them Event organizers typically market such events across a multitude of websites, each usually focusing on specific geographical areas, or overall topic groups. The volume of events can be somewhat daunting, leaving time constrained young professionals hearing about useful events only after they happen.

1.2. Our Solution

We plan to limit the amount of time searching for events by scraping major websites containing event ads, and displaying the most relevant events to our end user. Utilizing natural language processing techniques, we will score events based on relevance, sentiment, phrases used, and other key matches. If possible, we would also like to give our end user a leg up over the competition, and where possible, we will provide information on potential guests, corporate sponsors, and more. In summary, we want our end user to find the following after using this program:

Top most relevant events given an overall relevance score.

Descriptive analysis of the key words and phrases that influenced the relevant score.

Names of people and companies that may potentially attend.

2. Data Sources

 The following websites are used to find and extract events for further consideration:

Eventbrite: a popular website for companies and organizations to host live events in many niches

Platform Calgary: a Calgary-based technology organization that hosts many local tech and business events

Edmonton Regional Innovation Network (ERIN): a coalition of organizations in the Edmonton region that support entrepreneurship and business activity

Eventbrite contains a variety of available data, while Platform Calgary and ERIN represent specific locales and niches to compare against. 

3. Methodology

3.1. Web Scraping

To gather data, JavaScript files are written to scrape information from the events section of their respective websites. Each website's unique DOM is accessed to extract event titles, locations, dates and times as applicable. From there, individual pages under the events section are accessed to extract full event descriptions and links. Lastly, initial preprocessing is performed to extract the day of week. 

In the end, all upcoming events from ERIN and Platform Calgary are obtained, while all business events for the week ahead from Eventbrite are obtained. This process is flexible to run at any time period, and JSON outputs from each JavaScript file follow this data schema:

Platform (platform): what website the event appeared in

Title (title): the event's full title

Link (event_url): link to view and register in the event

Date (date): event date in mm/dd/yyyy format

Day of week (day_of_week)

Start time (start_time): when the event starts

End time (end_time): when the event ends

Location (location): where the event takes place

Description (description): the full event description

{r}
edmontonrin <- system("node ./edmontonrin.js", intern = TRUE) %>%
  fromJSON() %>%
  as_tibble()

{r}
platformcalgary <- system("node ./platformcalgary.js", intern = TRUE) %>%
  fromJSON() %>% 
  as_tibble()

{r,include = FALSE}
#eventbrite <- system("node ./eventbrite.js", intern = TRUE) %>%
 # fromJSON() %>% 
  #as_tibble()

Combining all scraped data together into a full table and labeling each extracted event, we are ready to analyze further. An example output of the key features is below:

{r}
# all_events_raw <- bind_rows(edmontonrin, platformcalgary, eventbrite) %>% 
all_events_raw <- bind_rows(edmontonrin, platformcalgary) %>% 
  dplyr::mutate(event_id = row_number()) %>%
  dplyr::select(event_id, dplyr::everything())


all_events_raw[1,]%>%
  gt::gt()

3.2. Named Entity Recognition

Named entity recognition is an important process in natural language processing to identify special tokens such as people, organizations, places, and topics. These specific entities represent potential connections, companies, and topics that are expected to appear, directly leading to informed decision-making for what events to attend. The spaCy package will be crucial for extracting such entities after preprocessing raw event descriptions to better parse extracted tokens.

{r}
# Preprocess description for spaCy
all_events <- all_events_raw %>%
  mutate(
    description_clean = description %>%
      str_replace_all("([a-z])(\\.[A-Z])", "\\1. \\2") %>% # add space after a period glued to a capital
      str_replace_all("([a-z])(,[A-Z])", "\\1, \\2") %>% # add space after a comma glued to a capital
      str_replace_all("([A-Za-z])'s", "\\1 's") %>% # ensure space before possessive â€™s
      str_replace_all("([A-Z]{2,})([A-Z][a-z])", "\\1 \\2") %>%  # split run-on capitals
      str_squish()
  )


Note: potentially include an example raw vs preprocessed description here to illustrate differences

We install and use the transformer model in spaCy to obtain the most accurate results possible. For people, organizations, and places, special entities are extracted and cleaned separately. For topics, applicable noun phrases that can potentially capture event relevance are extracted as various length n-grams.

{r}
# Install and prepare spaCy transformer model
spacy_install()
spacy_download_langmodel("en_core_web_trf")

{r}
# Initialize spaCy
spacy_initialize(model = "en_core_web_trf")

# NER extraction across all descriptions
ner_df <- spacy_extract_entity(
  all_events$description_clean,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% # Preprocessing dataframe before further cleaning
  tibble::as_tibble() %>% 
  filter(!str_detect(text, "@"), # Remove email addresses
         !str_detect(text, "https?://")) # Remove links

# Noun phrase extraction across all descriptions
np_df <- spacy_extract_nounphrases(
  all_events$description_clean,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% # Preprocessing dataframe before further cleaning
  tibble::as_tibble()

spacy_finalize()

Note: make a potential bar chart for showing how many tokens of each entity type show up -> reasoning for only retaining PERSON, ORG, GPE

Out of all possible types, spaCy represents our entities of interest (people, organizations, and places) as PERSON, ORG, and GPE. We develop a process to improve extraction quality by removing stop words and duplicate entities as well as restoring capitalization for organization acronyms and names to retain proper cases. Lastly, we indicate null for each event without some of these entities, which will be useful for later ranking event relevance in terms of details present.

{r}
# Keep useful entity types and regroup by category
keep_types <- c("PERSON", "ORG", "GPE")
entity_category_map <- tibble(
  entity_type = keep_types,
  category = c("people", "organizations", "places")
)

# Remove stop words and prepare clean entities for each event
process_entities <- function(ner_df) {
  ner_df %>%
    filter(ent_type %in% keep_types) %>%
    mutate(
      entity_id   = row_number(), # Label each event
      orig_entity = text # Save original text
    ) %>%
    unnest_tokens(token, text) %>%
    anti_join(stop_words, by = c("token" = "word")) %>%
    group_by(doc_id, ent_type, entity_id, orig_entity) %>%
    summarise(
      entity_clean_tokens = list(token), # Get list of clean entities for each event
      .groups = "drop"
    ) %>%
    mutate(
      entity_clean = map_chr(entity_clean_tokens, ~ str_c(.x, collapse = " ")),
      entity_clean = str_squish(entity_clean),
      entity_clean = if_else(
        str_detect(orig_entity, "[A-Z]{2,}") | str_detect(orig_entity, "[A-Za-z][A-Z]"),
        orig_entity, # keep acronyms/mixed case
        str_to_title(entity_clean) # normalize regular names
      )
    ) %>%
    filter(entity_clean != "") %>%
    distinct(doc_id, ent_type, entity_clean) %>%
    left_join(entity_category_map, by = c("ent_type" = "entity_type")) %>%
    group_by(doc_id, category) %>%
    summarise(
      entities = list(unique(entity_clean)), # deduplicated vectors
      .groups = "drop"
    ) %>% 
    pivot_wider(names_from = category, values_from = entities) %>% 
    mutate(across(c(people, organizations, places),
                  ~ map(.x, ~ if (length(.x) == 0) NA_character_ else .x)))
}

# Call function
entities_tbl <- process_entities(ner_df)

Similarly, we also clean noun phrases by removing stop words where necessary and recombining tokens based on event. Here, we remeasure each recombined entity's length to remove unigrams as they are not indicative of potential discussed topics.

{r}
# Remove stop words from extracted noun phrases
process_nounphrases <- function(np_df) {
  np_df %>% 
    unnest_tokens(output = word, input = text) %>% 
    anti_join(stop_words, by = "word") %>%
    group_by(doc_id, root_id, start_id, length) %>%  # Keys for a phrase
    summarise(clean_text = str_c(word, collapse = " "),
              length = n(),
              .groups = "drop") %>%
    filter(clean_text != "") %>% 
    filter(length > 1) %>% # Remove unigrams
    mutate(clean_text = str_squish(clean_text)) %>%
    group_by(doc_id) %>%
    summarise(noun_phrases = list(unique(clean_text)), .groups = "drop")
}

# Call function
nounphrases_tbl <- process_nounphrases(np_df)

Having extracted named entities and noun phrases, we can enrich our current events table with this additional information.

{r}
# Attach enriched info to combined events for downstream personas / visualizations
all_events_enriched <- all_events %>%
  left_join(entities_tbl, join_by("event_id" == "doc_id")) %>%
  left_join(nounphrases_tbl, join_by("event_id" == "doc_id"))

all_events_enriched

Note: can make a bar chart of how many events have full information vs missing entities, nounphrases or both. This can optionally be used downstream to complement relevance score calculation (time permitting)

3.3. Natural Language Processing (NLP)

The importance of any given event is subjective to the wants and needs of our end user. Through the use of our own lived experiences, we can manage a heuristic response to an event's details to decide whether or not it is relevant. To mimic this, we must define personas, and create lexicons, to measurably compare events to our end user.

Why and How to use NLP?

Generating a precise understanding of the topic of the event.

Differentiate topics among equally relevant events.

Defining criteria by which we can choose relevant events.

Visualizing distributions of event relevance, and rationalizing top 3 choices from it.

We begin by tokenizing each word in all the event descriptions and plotting their overall use to extrapolate information about trending event topics.

{r}

nlp_tokens <- all_events_raw %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description, token = "ngrams", n = 2)

nlp_token_1 <- all_events_raw %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description)%>%
  dplyr::anti_join(tidytext::stop_words)



# We need to remove stop words, I can do so by creating a new column per words, replacing stop words, and seeing what happens



nlp_tokens <- nlp_tokens %>% 
  tidyr::separate_wider_delim(word, delim = " ", names = c("item1",
                                                           "item2"
                                                           ))

# for loop to remove stop_words
for(i in 2:nrow(nlp_tokens)){
  for(j in 2:length(nlp_tokens)){
    
    if(nlp_tokens[i, j] %in% tidytext::stop_words$word){
      
      nlp_tokens[i,j] <- ""
      
    }else{
      nlp_tokens[i,j] <- nlp_tokens[i,j]
    }
  }
}

nlp_tokens <- nlp_tokens %>% 
  tidyr::unite(item1, 2:length(nlp_tokens),  sep = " ")%>%
  dplyr::transmute(event_id, phrase = item1)

nlp_tokens$phrase <- nlp_tokens$phrase %>% gsub("^\\s+|\\s+$", "", .)

nlp_tokens <- nlp_tokens %>% dplyr::filter(stri_count_words(phrase) >= 2) %>% unique()


rbind(

(nlp_tokens%>%
  count(phrase, name = "count")%>%
   dplyr::rename("token" = "phrase")%>%dplyr::mutate(type = "Phrase")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n = 15)),
(nlp_token_1%>%
   count(word, name = "count")%>%
   dplyr::rename("token" = "word")%>%dplyr::mutate(type = "word")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n=15))

)%>%
  ggplot2::ggplot(aes(x = reorder(token, count), y = count, fill = token))+
  geom_col()+
  theme(legend.position = "none",
        panel.grid = element_line(colour = "lightgrey", linewidth = 0.2),
        panel.background = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())+
  coord_flip()+
  facet_grid(.~type)+
  labs(x = "Tokens", y = "Mentions")

We note that there are multiple phrases that are potentially counted repeatedly within the same context. We also face a problem of non-descriptive phrases containing filler words not already stopped out. Differentiating contexts for n-grams >1 can prove quite difficult. To both side-step this problem, and enhance our analysis, we can rely on our enriched data frame to pull noun-phrases to evaluate trends.

{r}
np_clean <- np_df %>% 
    unnest_tokens(output = word, input = text) %>% 
    anti_join(stop_words, by = "word") %>%
    group_by(doc_id, root_id, start_id, length) %>%  # Keys for a phrase
    summarise(clean_text = str_c(word, collapse = " "),
              length = n(),
              .groups = "drop") %>%
    filter(clean_text != "") %>% 
    filter(length > 1) %>% # Remove unigrams
    mutate(clean_text = str_squish(clean_text))


rbind(
(np_clean%>%
  count(clean_text, name = "count")%>%
   dplyr::rename("token" = "clean_text")%>%dplyr::mutate(type = "Noun_Phrase")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n = 15)),
(nlp_token_1%>%
   count(word, name = "count")%>%
   dplyr::rename("token" = "word")%>%dplyr::mutate(type = "word")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n=15))

)%>%
  ggplot2::ggplot(aes(x = reorder(token, count), y = count, fill = token))+
  geom_col()+
  theme(legend.position = "none",
        panel.grid = element_line(colour = "lightgrey", linewidth = 0.2),
        panel.background = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())+
  coord_flip()+
  facet_grid(.~type)+
  labs(x = "Tokens", y = "Mentions")


At a high level, the information provided by the above graphics can inform the end user on trending topics in their area. What it does not do is materially move our analysis closer to pinpointing three relevant events. In order to make real headwinds, we cannot divorce our statistics from group level assignment.

A different high level view of relevant phrases is provided by the below graph:

{r}

np_igraph <- np_clean%>%
  dplyr::select(doc_id, clean_text)%>%
  igraph::graph_from_data_frame()
  
V(np_igraph)$type <- bipartite_mapping(np_igraph)$type
  
np_igraph %>%  
  ggraph(layout = "nicely")+
  geom_edge_link(show.legend = FALSE)+
  geom_node_point(aes(colour = type), size = 5)+
  geom_node_text(aes(label = name),
                 size = 4,
                 vjust = 1.3,
                 hjust = 0.5)+
  theme(legend.position = "none",
        panel.background = element_blank(),
        panel.border = element_rect(colour = "black"))


Given the above graphic, you can extract multiple key takeaways about the event landscape you are a part of. High levels of fragmentation would signify an overall lack of synchronicity among events. A more interconnected cobweb of key phrases and events would support the belief that there is a dominant trend.

We can observe some clustering of events allowing us to establish some inter-connectivity if any.

{r}
networking_words <- list(
  
  keywords = c("network","networking", "meet", "job", "work","friend",
               "relationship", "mixer","peer"),
  
  others = c("engage", "exchange","social", "mingle", "introduction", 
             "collaborate", "partnership", "discuss", "roundtable")
  
)


{r}
persona_finance_student <- list(
  
  keywords = c(
    "finance", "financial", "investment", "investing", "markets",
    "economics", "trading", "derivatives", "valuation", "analytics",
    "data", "economy", "portfolio", "risk", "venture", "credit", "debt",
    "banking", "capital", "wealth", "equity", "fund", "money"
  ),
  
  priority_keywords = c("finance", "internship", "trading", "analyst", "mentorship", "professional"),
  
  locations = c("virtual", "online"),
  
  event_types = c("networking", "career fair", "seminar")
)

{r}
persona_tech_student <- list(
  
  keywords = c(
    "tech", "technology", "software", "developer", "coding",
    "programming", "data", "python", "r", "cloud", "app",
    "machine", "learning", "ai", "intelligence", "artificial"
  ),
  
  priority_keywords = c(
    "hackathon", "developer", "internship", "programming"
  ),
  
  locations = c("calgary"),
  
  event_types = c("workshop", "webinar", "hackathon")
)


{r}
persona_entrepreneur <- list(
  
  keywords = c(
    "entrepreneur", "startup", "business", "innovation",
    "pitch", "founder", "funding", "capital", "growth", "strategy",
    "marketing", "strategy", "leadership", "sales", "money", "sales"
  ),
  
  priority_keywords = c(
    "sales", "pitch", "startup", "entrepreneur", "mentor"
  ),
  
  locations = c("virtual", "online"),
  
  event_types = c(
    "networking", "pitch", "seminar"
  )
)

Persona and Lexicon Matching

To score events by relevance, we match keywords in pre-defined lexicons containing preferred language, with each event's description. Below is a visual example of the first step in our machine learning workflow.

{r}


topic <- persona_finance_student$keywords # choose your topic of interest here

nlp_token_1$Contains <- "" # initializing the contains column 


for(i in 1:nrow(nlp_token_1)){

nlp_token_1$Contains[i] <- grepl(nlp_token_1[i,2], paste(topic, collapse = ""))

}

nlp_fit <- nlp_token_1 %>%
  dplyr::filter(Contains == "TRUE")%>%
  dplyr::group_by(event_id)%>%
  dplyr::count(word, name = "Matches")%>%
  dplyr::arrange(desc(Matches))

nlp_fit %>%
  dplyr::group_by(event_id)%>% dplyr::summarise(Matches = sum(Matches))%>%
  dplyr::arrange(desc(Matches))%>% dplyr::slice_head(n = 10)%>%
  ggplot2::ggplot(aes(x = Matches, y = reorder(event_id, Matches), fill = Matches))+
  geom_col()+
  theme(panel.background = element_blank(),
        panel.grid = element_line(colour = "lightgrey"),
        legend.position = "none")+
  labs(x = "Matches", y = "Event ID")





At its core, we count matches between defined criteria and event descriptions. In the above example, taking count as score, the top ranking event will thus be the recommended to our end user. This is a simplified example.

Bi-gram Matching

Expanding from the uni-gram analysis, phrases can provide extra relevance, especially if repeated.

{r}


np_pairwise <- np_clean %>%
  dplyr::select(doc_id, clean_text)%>%
  widyr::pairwise_cor(
    item = clean_text,
    feature = doc_id,
    sort = T,
    upper = FALSE)%>%
  dplyr::filter(correlation > .7)




  persona1 <- persona_finance_student
  
  np_pairwise$Contains = ""
  
  
  for(j in 1:length(persona1$keywords)){

    np_pairwise$Contains <-grepl(persona1$keywords[j], 
                                    np_pairwise$item1,
                                    ignore.case = TRUE)
  
    
    
  }



  
igraph::graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name),
                 size = 3,
                 vjust = 1,
                 hjust = 1) +
  theme_void()




np_pairwise

{r}

# Attempting same logic as matches first, **no use for lexicon dataframe
#Initializing dataframe (this is not AI btw)

nlp_type <- nlp_token_1
nlp_type$Networking <- ""



for(i in 1:nrow(nlp_type)){

nlp_type$Networking[i] <- grepl(nlp_type[i,2], paste(networking_words, collapse = ""))


}

nlp_type <- nlp_type %>%
  dplyr::mutate(across(3:ncol(nlp_type), ~ as.integer(as.logical(.))))


nlp_type %>%
  tidyr::pivot_longer(cols = c(4:ncol(nlp_type)),
                      names_to = "Type",
                      values_to = "Score")%>%
  dplyr::filter(Score != 0)%>%
  ggplot2::ggplot(aes(x = as.character(event_id), 
                      y = Score,
                      fill = Score))+
  geom_col()+
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.background = element_rect(fill = "lightgrey"),
        legend.position = "none")+
  labs(title = "Event Type Scores",
       )+
  coord_flip()
  


# Not generalized so far
nlp_type_scores <- nlp_type%>%
  dplyr::group_by(event_id)%>%
  dplyr::summarise(Network_Score = sum(Networking),
                   Matches = sum(Contains))%>%
  dplyr::filter(Matches == 1)


extract in multiple ways

if u extract and match to unigrams, those should indicate the terms that are used often. you can get a measure of relevance from unigram matches directly. (what we did)

with phrases, usually its very deliberate, could show extra relevance.

{r}


event_words <- nlp_type %>% dplyr::filter(Contains != 0)%>%
  dplyr::select(event_id)%>% left_join(., nlp_token_1, by = "event_id")%>%dplyr::group_by(event_id)%>%
  dplyr::count(word, name = "Instances")%>%
  dplyr::arrange(desc(Instances))



event_igraph <- event_words%>%  dplyr::group_by(word)%>%
  igraph::graph_from_data_frame()


event_igraph%>%
  ggraph(layout = "nicely")+
  geom_edge_link(show.legend = FALSE)+
  geom_node_point(colour = "orange", size = 5)+
  geom_node_text(aes(label = name),
                 size = 4,
                 vjust = 1.3,
                 hjust = 0.9)+
  theme_void()



Machine Learning Workflow

{r}
desc_tokens <- all_events %>%
  select(event_id, description) %>%
  filter(!is.na(description)) %>%
  tidytext::unnest_tokens(output = word, input = description) %>%
  anti_join(tidytext::stop_words)

title_tokens <- all_events %>%
  select(event_id, title) %>%
  filter(!is.na(title)) %>%
  tidytext::unnest_tokens(output = word, input = title) %>%
  anti_join(tidytext::stop_words, by = "word")


{r}
# DONE MATCH DESC SYNC WITH NLP
match_desc <- function(tokens, persona) {

  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }
# counting each instance per event that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Desc_Matches")
  
}

#__________________________________________________________________________________
# DONE MATCH TITLE SYNC WITH NLP
match_title <- function(tokens, persona) {
  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }

# counting each instance per event title that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Title_Matches")
}

#__________________________________________________________________________________

event_sentiment_key <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$keywords, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Key_Matches")


}

#__________________________________________________________________________________

event_sentiment_other <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$others, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Other_Matches")

}

#__________________________________________________________________________________


match_location <- function(events, persona) {
  events %>%
    mutate(
      location_lower = tolower(location),
      Location_Matches = if_else(
        str_detect(location_lower, str_c(persona$locations, collapse = "|")),
        1, 0, missing = 0)) %>%
    select(event_id, Location_Matches)
}


match_type <- function(events, persona) {
  events %>%
    mutate(
      desc_lower = tolower(description),
      Type_Matches = if_else(
        str_detect(desc_lower, str_c(persona$event_types, collapse = "|")),
        1L, 0L, missing = 0L)) %>%
    select(event_id, Type_Matches)
}

#__________________________________________________________________________________

match_highly_relevant <- function(){
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
}







{r}
build_event_features <- function(all_events, desc_tokens, title_tokens, 
                                 persona, networking_words) {
  
  desc_hits_tbl     <- match_desc(desc_tokens, persona)
  title_hits_tbl    <- match_title(title_tokens, persona)
  sentiment_key_tbl <- event_sentiment_key(desc_tokens, networking_words)
  sentiment_other_tbl <- event_sentiment_other(desc_tokens, networking_words)
  loc_match_tbl     <- match_location(all_events, persona)
  type_match_tbl    <- match_type(all_events, persona)
  
  all_events %>%
    left_join(desc_hits_tbl,     by = "event_id") %>%
    left_join(title_hits_tbl, by = "event_id") %>%
    left_join(sentiment_key_tbl,    by = "event_id") %>%
    left_join(sentiment_other_tbl,     by = "event_id") %>%
    left_join(loc_match_tbl,     by = "event_id") %>%
    left_join(type_match_tbl,    by = "event_id") %>%
    mutate(
      Desc_Matches          = replace_na(Desc_Matches, 0),
      Title_Matches         = replace_na(Title_Matches, 0),
      Sentiment_Key_Matches       = replace_na(Sentiment_Key_Matches, 0),
      Sentiment_Other_Matches       = replace_na(Sentiment_Other_Matches, 0),
      Location_Matches      = replace_na(Location_Matches, 0),
      Type_Matches          = replace_na(Type_Matches, 0)
    )
}

{r}
score_event_relevance <- function(events_features) {
  
  scored <- events_features %>%
    mutate(
      raw_score =
        1.0 * Desc_Matches +
        1.5 * Title_Matches +
        1.5 * Sentiment_Key_Matches +
        0.2 * Sentiment_Other_Matches +
        1.0 * Location_Matches +
        1.0 * Type_Matches
    )
  
  min_val <- min(scored$raw_score, na.rm = TRUE)
  max_val <- max(scored$raw_score, na.rm = TRUE)
  
  scored %>%
    mutate(relevance_score = round(1 + 9 * ((raw_score - min_val) / (max_val - min_val)), 2)) %>%
    
    arrange(desc(relevance_score))
}


{r}
get_top_events <- function(events_scored, n = 3) {
  events_scored %>%
    slice_head(n = n) %>%
    select(
      title,
      date,
      start_time,
      end_time,
      location,
      description,
      event_url,
      relevance_score
    )
}

{r}
events_features_1 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_finance_student,
  networking_words
)

events_scored_1 <- score_event_relevance(events_features_1)
top3_events_1 <- get_top_events(events_scored_1, 3)
top3_events_1


{r}
events_features_2 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_tech_student,
  networking_words
)

events_scored_2 <- score_event_relevance(events_features_2)
top3_events_2 <- get_top_events(events_scored_2, 3)
top3_events_2

{r}
events_features_3 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_entrepreneur,
  networking_words
)

events_scored_3 <- score_event_relevance(events_features_3)
top3_events_3 <- get_top_events(events_scored_3, 3)
top3_events_3

What Next:

Rough Structure of Report. Section where each visualization will belong.

Report:

Intro: Framing the Problem/hypothesis

Problems with the job search, and proposal

