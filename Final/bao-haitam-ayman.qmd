---
title: "Event Scraper for In-Person Networking"
author: "Bao Nguyen, Haitam Aksikas, Ayman Arman"
format: html
editor: visual
---

## 

```{r, include = FALSE}

knitr::opts_chunk$set(echo = FALSE,
                  warning = FALSE,
                  message = FALSE)

```

```{r}
library(dplyr) 
library(tidyr) 
library(purrr) 
library(stringr) 
library(tibble) 
library(arrow) 
library(tidyverse) 
library(spacyr) 
library(textdata) 
library(jsonlite) 
library(tidytext)
library(igraph)
library(ggraph)
library(gt)
library(stringr) 
library(stringi) 
```

# Introduction

Job seekers are focusing more on securing internships or new graduate roles through strong connections and sponsors within their field. Young professionals are increasingly reliant on open invitation networking events to connect with individuals and begin forming a robust network around them Event organizers typically market such events across a multitude of websites, each usually focusing on specific geographical areas, or overall topic groups. The volume of events can be somewhat daunting, leaving time constrained young professionals hearing about useful events only after they happen.

### [Our Solution]{.underline}

We plan to limit the amount of time searching for events by scraping major websites containing event ads, and displaying the most relevant events to our end user. The websites used in the development process are:

-   Eventbrite

-   Platform Calgary

-   Edmonton RIN

Utilizing natural language processing techniques, we will score events based on relevance, sentiment, phrases used, and other key matches. If possible, we would also like to give our end user a leg up over the competition, and where possible, we will provide information on potential guests, corporate sponsors, and more. In summary, we want our end user to find the following after using this program:

1.  Top most relevant events given an overall relevance score.
2.  Descriptive analysis of the key words and phrases that influenced the relevant score.
3.  Names of people and companies that may potentially attend.

```{r}
edmontonrin <- system("node ./edmontonrin.js", intern = TRUE) %>%
  fromJSON() %>%
  as_tibble()
```

```{r}
platformcalgary <- system("node ./platformcalgary.js", intern = TRUE) %>%
  fromJSON() %>% 
  as_tibble()
```

```{r,include = FALSE}
#eventbrite <- system("node ./eventbrite.js", intern = TRUE) %>%
 # fromJSON() %>% 
  #as_tibble()
```

# Methodology

The data is wrangled using a javascript web scraper. An example of the key features and a real output are below:

```{r}
all_events_raw <- bind_rows(edmontonrin, platformcalgary) %>% 
  dplyr::mutate(event_id = row_number()) %>%
  dplyr::select(event_id, dplyr::everything())


all_events_raw[1,]%>%
  gt::gt()
```

\*\* NER EXPLANATION HERE EXPLAINING THE UPDATED FEATURE LIST \*\*

```{r}
# Process NER
# Load from feather file (replace with actual tibble later)
# Preprocess description for spaCy
all_events <- all_events_raw %>% 
  mutate(
    description_clean = description %>%
      str_replace_all("([a-z])(\\.[A-Z])", "\\1. \\2") %>%   # add space after a period glued to a capital
      str_replace_all("([a-z])(,[A-Z])", "\\1, \\2") %>%     # add space after a comma glued to a capital
      str_replace_all("([A-Za-z])'s", "\\1 's") %>%          # ensure space before possessive â€™s
      str_replace_all("([A-Z]{2,})([A-Z][a-z])", "\\1 \\2") %>%  # split run-on capitals
      str_squish()
  )


# Initialize spaCy
spacy_install()
spacy_download_langmodel("en_core_web_sm")
spacy_initialize(model = "en_core_web_sm")

# Run NER + noun phrases across all descriptions
ner_df <- spacy_extract_entity(
  all_events$description,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% 
  tibble::as_tibble()

# Preprocessing NER dataframe before further cleaning
ner_df_clean <- ner_df %>%
  filter(!str_detect(text, "@"),
         !str_detect(text, "https?://"))

np_df <- spacy_extract_nounphrases(
  all_events$description,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% 
  tibble::as_tibble()


spacy_finalize()

# Keep useful entity types and regroup by category
keep_types <- c("PERSON", "ORG", "GPE")
entity_category_map <- tibble(
  entity_type = keep_types,
  category = c("people", "organizations", "places")
)

# Test entity cleanup (deduplication and normalization)
entities_tbl <- ner_df_clean %>%
  filter(ent_type %in% keep_types) %>%
  mutate(
    entity_id   = row_number(),     # remember each original span
    orig_entity = text              # save original casing
  ) %>%
  unnest_tokens(token, text) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  group_by(doc_id, ent_type, entity_id, orig_entity) %>%
  summarise(
    entity_clean_tokens = list(token),
    .groups = "drop"
  ) %>%
  mutate(
    token_count = lengths(entity_clean_tokens),
    entity_clean = map_chr(entity_clean_tokens, ~ str_c(.x, collapse = " ")),
    entity_clean = str_squish(entity_clean),
    entity_clean = if_else(
      str_detect(orig_entity, "[A-Z]{2,}") | str_detect(orig_entity, "[A-Za-z][A-Z]"),
      orig_entity,                          # keep acronyms/mixed case
      str_to_title(entity_clean)            # normalize regular names
    )
  ) %>%
  filter(entity_clean != "") %>%
  distinct(doc_id, ent_type, entity_clean) %>%
  left_join(entity_category_map, by = c("ent_type" = "entity_type")) %>%
  group_by(doc_id, category) %>%
  summarise(
    entities = list(unique(entity_clean)),   # deduplicated vectors
    .groups = "drop"
  )

entities_tbl_wide <- entities_tbl %>% 
  pivot_wider(names_from = category, values_from = entities)


# Clean up extracted named entities
ner_tokens <- ner_df %>% 
  unnest_tokens(output = word, input = text) %>% 
  anti_join(stop_words, by = "word")

ner_clean <- ner_tokens %>%
  group_by(doc_id, start_id, length) %>%  # keys for an entity
  summarise(clean_text = str_c(word, collapse = " "),
            length = n(),
            .groups = "drop")




# Clean up extracted noun phrases
np_tokens <- np_df %>% 
  unnest_tokens(output = word, input = text) %>% 
  anti_join(stop_words, by = "word")

np_clean <- np_tokens %>%
  group_by(doc_id, root_id, start_id, length) %>%  # keys for a phrase
  summarise(clean_text = str_c(word, collapse = " "),
            length = n(),
            .groups = "drop") %>%
  filter(clean_text != "") %>% 
  filter(length > 1)

nounphrases_tbl <- np_clean %>%
  mutate(clean_text = str_squish(clean_text)) %>%
  group_by(doc_id) %>%
  summarise(noun_phrases = list(unique(clean_text)), .groups = "drop")

# Attach enriched info to combined events for downstream personas / visualizations
all_events_enriched <- all_events %>%
  left_join(entities_tbl_wide,   join_by("event_id" == "doc_id")) %>%
  left_join(nounphrases_tbl, join_by("event_id" == "doc_id"))

all_events_enriched

```

## Natural Language Processing (NLP)

The importance of any given event is subjective to the wants and needs of our end user. Through the use of our own lived experiences, we can manage a heuristic response to an event's details to decide whether or not it is relevant. To mimic this, we must define personas, and create lexicons, to measurably compare events to our end user.

### Why and How to use NLP?

-   *Generating a precise understanding of the topic of the event.*

-   Differentiate topics among equally relevant events.

-   *Defining criteria by which we can choose relevant events.*

-   Visualizing distributions of event relevance, and rationalizing top 3 choices from it.

We begin by tokenizing each word in all the event descriptions and plotting their overall use to extrapolate information about trending event topics.

```{r}

nlp_tokens <- all_events_raw %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description, token = "ngrams", n = 2)

nlp_token_1 <- all_events_raw %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description)%>%
  dplyr::anti_join(tidytext::stop_words)



# We need to remove stop words, I can do so by creating a new column per words, replacing stop words, and seeing what happens



nlp_tokens <- nlp_tokens %>% 
  tidyr::separate_wider_delim(word, delim = " ", names = c("item1",
                                                           "item2"
                                                           ))

# for loop to remove stop_words
for(i in 2:nrow(nlp_tokens)){
  for(j in 2:length(nlp_tokens)){
    
    if(nlp_tokens[i, j] %in% tidytext::stop_words$word){
      
      nlp_tokens[i,j] <- ""
      
    }else{
      nlp_tokens[i,j] <- nlp_tokens[i,j]
    }
  }
}

nlp_tokens <- nlp_tokens %>% 
  tidyr::unite(item1, 2:length(nlp_tokens),  sep = " ")%>%
  dplyr::transmute(event_id, phrase = item1)

nlp_tokens$phrase <- nlp_tokens$phrase %>% gsub("^\\s+|\\s+$", "", .)

nlp_tokens <- nlp_tokens %>% dplyr::filter(stri_count_words(phrase) >= 2) %>% unique()


rbind(

(nlp_tokens%>%
  count(phrase, name = "count")%>%
   dplyr::rename("token" = "phrase")%>%dplyr::mutate(type = "Phrase")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n = 15)),
(nlp_token_1%>%
   count(word, name = "count")%>%
   dplyr::rename("token" = "word")%>%dplyr::mutate(type = "word")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n=15))

)%>%
  ggplot2::ggplot(aes(x = reorder(token, count), y = count, fill = token))+
  geom_col()+
  theme(legend.position = "none",
        panel.grid = element_line(colour = "lightgrey", linewidth = 0.2),
        panel.background = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())+
  coord_flip()+
  facet_grid(.~type)+
  labs(x = "Tokens", y = "Mentions")
```

We note that there are multiple phrases that are potentially counted repeatedly within the same context. We also face a problem of non-descriptive phrases containing filler words not already stopped out. Differentiating contexts for n-grams \>1 can prove quite difficult. To both side-step this problem, and enhance our analysis, we can rely on our enriched data frame to pull noun-phrases to evaluate trends.

```{r}



rbind(
(np_clean%>%
  count(clean_text, name = "count")%>%
   dplyr::rename("token" = "clean_text")%>%dplyr::mutate(type = "Noun_Phrase")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n = 15)),
(nlp_token_1%>%
   count(word, name = "count")%>%
   dplyr::rename("token" = "word")%>%dplyr::mutate(type = "word")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n=15))

)%>%
  ggplot2::ggplot(aes(x = reorder(token, count), y = count, fill = token))+
  geom_col()+
  theme(legend.position = "none",
        panel.grid = element_line(colour = "lightgrey", linewidth = 0.2),
        panel.background = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())+
  coord_flip()+
  facet_grid(.~type)+
  labs(x = "Tokens", y = "Mentions")

```

At a high level, the information provided by the above graphics can inform the end user on trending topics in their area. What it does not do is materially move our analysis closer to pinpointing three relevant events. In order to make real headwinds, we cannot divorce our statistics from group level assignment.

A different high level view of relevant phrases is provided by the below graph:

```{r}

np_igraph <- np_clean%>%
  dplyr::select(doc_id, clean_text)%>%
  igraph::graph_from_data_frame()
  
V(np_igraph)$type <- bipartite_mapping(np_igraph)$type
  
np_igraph %>%  
  ggraph(layout = "nicely")+
  geom_edge_link(show.legend = FALSE)+
  geom_node_point(aes(colour = type), size = 5)+
  geom_node_text(aes(label = name),
                 size = 4,
                 vjust = 1.3,
                 hjust = 0.5)+
  theme(legend.position = "none",
        panel.background = element_blank(),
        panel.border = element_rect(colour = "black"))

```

Given the above graphic, you can extract multiple key takeaways about the event landscape you are a part of. High levels of fragmentation would signify an overall lack of synchronicity among events. A more interconnected cobweb of key phrases and events would support the belief that there is a dominant trend.

We can observe some clustering of events allowing us to establish some inter-connectivity if any.

```{r}
networking_words <- list(
  
  keywords = c("network","networking", "meet", "job", "work","friend",
               "relationship", "mixer","peer"),
  
  others = c("engage", "exchange","social", "mingle", "introduction", 
             "collaborate", "partnership", "discuss", "roundtable")
  
)

```

```{r}
persona_finance_student <- list(
  
  keywords = c(
    "finance", "financial", "investment", "investing", "markets",
    "economics", "trading", "derivatives", "valuation", "analytics",
    "data", "economy", "portfolio", "risk", "venture", "credit", "debt",
    "banking", "capital", "wealth", "equity", "fund", "money"
  ),
  
  priority_keywords = c("finance", "internship", "trading", "analyst", "mentorship", "professional"),
  
  locations = c("virtual", "online"),
  
  event_types = c("networking", "career fair", "seminar")
)
```

```{r}
persona_tech_student <- list(
  
  keywords = c(
    "tech", "technology", "software", "developer", "coding",
    "programming", "data", "python", "r", "cloud", "app",
    "machine", "learning", "ai", "intelligence", "artificial"
  ),
  
  priority_keywords = c(
    "hackathon", "developer", "internship", "programming"
  ),
  
  locations = c("calgary"),
  
  event_types = c("workshop", "webinar", "hackathon")
)

```

```{r}
persona_entrepreneur <- list(
  
  keywords = c(
    "entrepreneur", "startup", "business", "innovation",
    "pitch", "founder", "funding", "capital", "growth", "strategy",
    "marketing", "strategy", "leadership", "sales", "money", "sales"
  ),
  
  priority_keywords = c(
    "sales", "pitch", "startup", "entrepreneur", "mentor"
  ),
  
  locations = c("virtual", "online"),
  
  event_types = c(
    "networking", "pitch", "seminar"
  )
)
```

### Persona and Lexicon Matching

To score events by relevance, we match keywords in pre-defined lexicons containing preferred language, with each event's description. Below is a visual example of the first step in our machine learning workflow.

```{r}


topic <- persona_finance_student$keywords # choose your topic of interest here

nlp_token_1$Contains <- "" # initializing the contains column 


for(i in 1:nrow(nlp_token_1)){

nlp_token_1$Contains[i] <- grepl(nlp_token_1[i,2], paste(topic, collapse = ""))

}

nlp_fit <- nlp_token_1 %>%
  dplyr::filter(Contains == "TRUE")%>%
  dplyr::group_by(event_id)%>%
  dplyr::count(word, name = "Matches")%>%
  dplyr::arrange(desc(Matches))

nlp_fit %>%
  dplyr::group_by(event_id)%>% dplyr::summarise(Matches = sum(Matches))%>%
  dplyr::arrange(desc(Matches))%>% dplyr::slice_head(n = 10)%>%
  ggplot2::ggplot(aes(x = Matches, y = reorder(event_id, Matches), fill = Matches))+
  geom_col()+
  theme(panel.background = element_blank(),
        panel.grid = element_line(colour = "lightgrey"),
        legend.position = "none")+
  labs(x = "Matches", y = "Event ID")




```

At its core, we count matches between defined criteria and event descriptions. In the above example, taking count as score, the top ranking event will thus be the recommended to our end user. This is a simplified example.

### Bi-gram Matching

Expanding from the uni-gram analysis, phrases can provide extra relevance, especially if repeated.

```{r}


np_pairwise <- np_clean %>%
  dplyr::select(doc_id, clean_text)%>%
  widyr::pairwise_cor(
    item = clean_text,
    feature = doc_id,
    sort = T,
    upper = FALSE)%>%
  dplyr::filter(correlation > .7)




  persona1 <- persona_finance_student
  
  np_pairwise$Contains = ""
  
  
  for(j in 1:length(persona1$keywords)){

    np_pairwise$Contains <-grepl(persona1$keywords[j], 
                                    np_pairwise$item1,
                                    ignore.case = TRUE)
  
    
    
  }



  
igraph::graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name),
                 size = 3,
                 vjust = 1,
                 hjust = 1) +
  theme_void()




np_pairwise
```

```{r}

# Attempting same logic as matches first, **no use for lexicon dataframe
#Initializing dataframe (this is not AI btw)

nlp_type <- nlp_token_1
nlp_type$Networking <- ""



for(i in 1:nrow(nlp_type)){

nlp_type$Networking[i] <- grepl(nlp_type[i,2], paste(networking_words, collapse = ""))


}

nlp_type <- nlp_type %>%
  dplyr::mutate(across(3:ncol(nlp_type), ~ as.integer(as.logical(.))))


nlp_type %>%
  tidyr::pivot_longer(cols = c(4:ncol(nlp_type)),
                      names_to = "Type",
                      values_to = "Score")%>%
  dplyr::filter(Score != 0)%>%
  ggplot2::ggplot(aes(x = as.character(event_id), 
                      y = Score,
                      fill = Score))+
  geom_col()+
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.background = element_rect(fill = "lightgrey"),
        legend.position = "none")+
  labs(title = "Event Type Scores",
       )+
  coord_flip()
  


# Not generalized so far
nlp_type_scores <- nlp_type%>%
  dplyr::group_by(event_id)%>%
  dplyr::summarise(Network_Score = sum(Networking),
                   Matches = sum(Contains))%>%
  dplyr::filter(Matches == 1)

```

-   extract in multiple ways

-   if u extract and match to unigrams, those should indicate the terms that are used often. you can get a measure of relevance from unigram matches directly. (what we did)

-   with phrases, usually its very deliberate, could show extra relevance.

```{r}


event_words <- nlp_type %>% dplyr::filter(Contains != 0)%>%
  dplyr::select(event_id)%>% left_join(., nlp_token_1, by = "event_id")%>%dplyr::group_by(event_id)%>%
  dplyr::count(word, name = "Instances")%>%
  dplyr::arrange(desc(Instances))



event_igraph <- event_words%>%  dplyr::group_by(word)%>%
  igraph::graph_from_data_frame()


event_igraph%>%
  ggraph(layout = "nicely")+
  geom_edge_link(show.legend = FALSE)+
  geom_node_point(colour = "orange", size = 5)+
  geom_node_text(aes(label = name),
                 size = 4,
                 vjust = 1.3,
                 hjust = 0.9)+
  theme_void()


```

### Machine Learning Workflow

```{r}
desc_tokens <- all_events %>%
  select(event_id, description) %>%
  filter(!is.na(description)) %>%
  tidytext::unnest_tokens(output = word, input = description) %>%
  anti_join(tidytext::stop_words)

title_tokens <- all_events %>%
  select(event_id, title) %>%
  filter(!is.na(title)) %>%
  tidytext::unnest_tokens(output = word, input = title) %>%
  anti_join(tidytext::stop_words, by = "word")

```

```{r}
# DONE MATCH DESC SYNC WITH NLP
match_desc <- function(tokens, persona) {

  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }
# counting each instance per event that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Desc_Matches")
  
}

#__________________________________________________________________________________
# DONE MATCH TITLE SYNC WITH NLP
match_title <- function(tokens, persona) {
  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }

# counting each instance per event title that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Title_Matches")
}

#__________________________________________________________________________________

event_sentiment_key <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$keywords, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Key_Matches")


}

#__________________________________________________________________________________

event_sentiment_other <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$others, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Other_Matches")

}

#__________________________________________________________________________________


match_location <- function(events, persona) {
  events %>%
    mutate(
      location_lower = tolower(location),
      Location_Matches = if_else(
        str_detect(location_lower, str_c(persona$locations, collapse = "|")),
        1, 0, missing = 0)) %>%
    select(event_id, Location_Matches)
}


match_type <- function(events, persona) {
  events %>%
    mutate(
      desc_lower = tolower(description),
      Type_Matches = if_else(
        str_detect(desc_lower, str_c(persona$event_types, collapse = "|")),
        1L, 0L, missing = 0L)) %>%
    select(event_id, Type_Matches)
}

#__________________________________________________________________________________

match_highly_relevant <- function(){
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
}






```

```{r}
build_event_features <- function(all_events, desc_tokens, title_tokens, 
                                 persona, networking_words) {
  
  desc_hits_tbl     <- match_desc(desc_tokens, persona)
  title_hits_tbl    <- match_title(title_tokens, persona)
  sentiment_key_tbl <- event_sentiment_key(desc_tokens, networking_words)
  sentiment_other_tbl <- event_sentiment_other(desc_tokens, networking_words)
  loc_match_tbl     <- match_location(all_events, persona)
  type_match_tbl    <- match_type(all_events, persona)
  
  all_events %>%
    left_join(desc_hits_tbl,     by = "event_id") %>%
    left_join(title_hits_tbl, by = "event_id") %>%
    left_join(sentiment_key_tbl,    by = "event_id") %>%
    left_join(sentiment_other_tbl,     by = "event_id") %>%
    left_join(loc_match_tbl,     by = "event_id") %>%
    left_join(type_match_tbl,    by = "event_id") %>%
    mutate(
      Desc_Matches          = replace_na(Desc_Matches, 0),
      Title_Matches         = replace_na(Title_Matches, 0),
      Sentiment_Key_Matches       = replace_na(Sentiment_Key_Matches, 0),
      Sentiment_Other_Matches       = replace_na(Sentiment_Other_Matches, 0),
      Location_Matches      = replace_na(Location_Matches, 0),
      Type_Matches          = replace_na(Type_Matches, 0)
    )
}
```

```{r}
score_event_relevance <- function(events_features) {
  
  scored <- events_features %>%
    mutate(
      raw_score =
        1.0 * Desc_Matches +
        1.5 * Title_Matches +
        1.5 * Sentiment_Key_Matches +
        0.2 * Sentiment_Other_Matches +
        1.0 * Location_Matches +
        1.0 * Type_Matches
    )
  
  min_val <- min(scored$raw_score, na.rm = TRUE)
  max_val <- max(scored$raw_score, na.rm = TRUE)
  
  scored %>%
    mutate(relevance_score = round(1 + 9 * ((raw_score - min_val) / (max_val - min_val)), 2)) %>%
    
    arrange(desc(relevance_score))
}

```

```{r}
get_top_events <- function(events_scored, n = 3) {
  events_scored %>%
    slice_head(n = n) %>%
    select(
      title,
      date,
      start_time,
      end_time,
      location,
      description,
      event_url,
      relevance_score
    )
}
```

```{r}
events_features_1 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_finance_student,
  networking_words
)

events_scored_1 <- score_event_relevance(events_features_1)
top3_events_1 <- get_top_events(events_scored_1, 3)
top3_events_1

```

```{r}
events_features_2 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_tech_student,
  networking_words
)

events_scored_2 <- score_event_relevance(events_features_2)
top3_events_2 <- get_top_events(events_scored_2, 3)
top3_events_2
```

```{r}
events_features_3 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_entrepreneur,
  networking_words
)

events_scored_3 <- score_event_relevance(events_features_3)
top3_events_3 <- get_top_events(events_scored_3, 3)
top3_events_3
```

What Next:

Rough Structure of Report. Section where each visualization will belong.

Report:

-   Intro: Framing the Problem/hypothesis
    -   Problems with the job search, and proposal
