---
title: "Event Scraper for In-Person Networking"
author: "Bao Nguyen, Haitam Aksikas, Ayman Arman"
format: html
editor: visual
---

## 

```{r, include = FALSE}

knitr::opts_chunk$set(echo = FALSE,
                  warning = FALSE,
                  message = FALSE)

```

```{r}
library(dplyr) 
library(tidyr) 
library(purrr) 
library(stringr) 
library(tibble) 
library(arrow) 
library(tidyverse) 
library(spacyr) 
library(textdata) 
library(jsonlite) 
library(tidytext)
library(igraph)
library(ggraph)
library(gt)
library(stringr) 
library(stringi) 
```

# 1. Introduction

Job seekers are focusing more on securing internships or new graduate roles through strong connections and sponsors within their field. The typical student may send out numerous cold emails each week, with only a small fraction eventually translating into some form of dialogue. To skip the hassle, and develop meaningful relationships, young professionals are increasingly reliant on open invitation networking events to connect in-person and begin forming robust networks around them. Event organizers typically market such events across a multitude of websites, each usually focusing on specific geographical areas, or overall topic groups. Unfortunately, the sheer volume of events can be somewhat daunting to analyze, leaving time constrained young professionals hearing about useful events only after they happen.

### 1.2. Our Solution

We plan to limit the amount of time searching for events by scraping major websites containing event ads, and displaying the most relevant events to our end user.

Utilizing natural language processing techniques, we will score events based on relevance, sentiment, phrases used, and other key matches. If possible, we would also like to give our end user a leg up over the competition, and where possible, we will provide information on potential guests, corporate sponsors, and more. In summary, we want our end user to find the following after using this program:

1.  Top most relevant events given an overall relevance score.
2.  Descriptive analysis of the key words and phrases that influenced the relevant score.
3.  Names of people and companies that may potentially attend.

# 2. Data Sources

The following websites are used to find and extract events for further consideration:

-   Eventbrite: a popular website for companies and organizations to host live events in many niches

-   Platform Calgary: a Calgary-based technology organization that hosts many local tech and business events

-   Edmonton Regional Innovation Network (ERIN): a coalition of organizations in the Edmonton region that support entrepreneurship and business activity

Eventbrite contains a variety of available data, while Platform Calgary and ERIN represent specific locales and niches to compare against.

```{r}
edmontonrin <- system("node ./edmontonrin.js", intern = TRUE) %>%
  fromJSON() %>%
  as_tibble()
```

```{r}
platformcalgary <- system("node ./platformcalgary.js", intern = TRUE) %>%
  fromJSON() %>% 
  as_tibble()
```

```{r,include = FALSE}
#eventbrite <- system("node ./eventbrite.js", intern = TRUE) %>%
 # fromJSON() %>% 
  #as_tibble()
```

# 3. Methodology

### 3.1. Web Scraping

To gather data, JavaScript files are written to scrape information from the events section of their respective websites. Each website's unique DOM is accessed to extract event titles, locations, dates and times as applicable. From there, individual pages under the events section are accessed to extract full event descriptions and links. Lastly, initial preprocessing is performed to extract the day of week.

In the end, all upcoming events from ERIN and Platform Calgary are obtained, while all business events for the week ahead from Eventbrite are obtained. This process is flexible to run at any time period, and JSON outputs from each JavaScript file follow this data schema:

-   Platform (platform): what website the event appeared in

-   Title (title): the event's full title

-   Link (event_url): link to view and register in the event

-   Date (date): event date in mm/dd/yyyy format

-   Day of week (day_of_week)

-   Start time (start_time): when the event starts

-   End time (end_time): when the event ends

-   Location (location): where the event takes place

-   Description (description): the full event description

An example output is shown below:

```{r}
all_events_raw <- bind_rows(edmontonrin, platformcalgary) %>% 
  dplyr::mutate(event_id = row_number()) %>%
  dplyr::select(event_id, dplyr::everything())


all_events_raw[1,]%>%
  gt::gt()
```

### 3.2. Named Entity Recognition

Named entity recognition is an important process in natural language processing to identify special tokens such as people, organizations, places, and topics. These specific entities represent potential connections, companies, and topics that are expected to appear, directly leading to informed decision-making for what events to attend. The spaCy package will be crucial for extracting such entities after preprocessing raw event descriptions to better parse extracted tokens.

```{r}
# Preprocess description for spaCy
all_events <- all_events_raw %>%
  mutate(
    description_clean = description %>%
      str_replace_all("([a-z])(\\.[A-Z])", "\\1. \\2") %>%   # add space after a period glued to a capital
      str_replace_all("([a-z])(,[A-Z])", "\\1, \\2") %>%     # add space after a comma glued to a capital
      str_replace_all("([A-Za-z])'s", "\\1 's") %>%          # ensure space before possessive ’s
      str_replace_all("([A-Z]{2,})([A-Z][a-z])", "\\1 \\2") %>%  # split run-on capitals
      str_squish()
  )

```

We install and use the transformer model in spaCy to obtain the most accurate results possible. For people, organizations, and places, special entities are extracted and cleaned separately. For topics, applicable noun phrases that can potentially capture event relevance are extracted as various length n-grams.

```{r}
# Install and prepare spaCy transformer model
spacy_install()
spacy_download_langmodel("en_core_web_trf")
```

Out of all possible types, spaCy represents our entities of interest (people, organizations, and places) as PERSON, ORG, and GPE. We develop a process to improve extraction quality by removing stop words and duplicate entities as well as restoring capitalization for organization acronyms and names to retain proper cases. Lastly, we indicate null for each event without some of these entities, which will be useful for later ranking event relevance in terms of details present.

Similarly, we also clean noun phrases by removing stop words where necessary and recombining tokens based on event. Here, we remeasure each recombined entity's length to remove unigrams as they are not indicative of potential discussed topics.

```{r}
# Initialize spaCy
spacy_initialize(model = "en_core_web_trf")

# NER extraction across all descriptions
ner_df <- spacy_extract_entity(
  all_events$description,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% 
  tibble::as_tibble() %>% # Preprocessing dataframe before further cleaning
  filter(!str_detect(text, "@"), # Remove email addresses
         !str_detect(text, "https?://")) # Remove links

# Noun phrase extraction across all descriptions
np_df <- spacy_extract_nounphrases(
  all_events$description,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% 
  tibble::as_tibble()

spacy_finalize()

# Keep useful entity types and regroup by category
keep_types <- c("PERSON", "ORG", "GPE")
entity_category_map <- tibble(
  entity_type = keep_types,
  category = c("people", "organizations", "places")
)

# Remove stop words and prepare clean entities for each event
process_entities <- function(ner_df) {
  ner_df %>%
    filter(ent_type %in% keep_types) %>%
    mutate(
      entity_id   = row_number(), # Label each event
      orig_entity = text # Save original text
    ) %>%
    unnest_tokens(token, text) %>%
    anti_join(stop_words, by = c("token" = "word")) %>%
    group_by(doc_id, ent_type, entity_id, orig_entity) %>%
    summarise(
      entity_clean_tokens = list(token), # Get list of clean entities for each event
      .groups = "drop"
    ) %>%
    mutate(
      entity_clean = map_chr(entity_clean_tokens, ~ str_c(.x, collapse = " ")),
      entity_clean = str_squish(entity_clean),
      entity_clean = if_else(
        str_detect(orig_entity, "[A-Z]{2,}") | str_detect(orig_entity, "[A-Za-z][A-Z]"),
        orig_entity, # keep acronyms/mixed case
        str_to_title(entity_clean) # normalize regular names
      )
    ) %>%
    filter(entity_clean != "") %>%
    distinct(doc_id, ent_type, entity_clean) %>%
    left_join(entity_category_map, by = c("ent_type" = "entity_type")) %>%
    group_by(doc_id, category) %>%
    summarise(
      entities = list(unique(entity_clean)), # deduplicated vectors
      .groups = "drop"
    ) %>% 
    pivot_wider(names_from = category, values_from = entities) %>% 
    mutate(across(c(people, organizations, places),
                  ~ map(.x, ~ if (length(.x) == 0) NA_character_ else .x)))
}

# Call function
entities_tbl <- process_entities(ner_df)

# Remove stop words from extracted noun phrases
process_nounphrases <- function(np_df) {
  np_df %>% 
    unnest_tokens(output = word, input = text) %>% 
    anti_join(stop_words, by = "word") %>%
    group_by(doc_id, root_id, start_id, length) %>%  # keys for a phrase
    summarise(clean_text = str_c(word, collapse = " "),
              length = n(),
              .groups = "drop") %>%
    filter(clean_text != "") %>% 
    filter(length > 1) %>%
    mutate(clean_text = str_squish(clean_text)) %>%
    group_by(doc_id) %>%
    summarise(noun_phrases = list(unique(clean_text)), .groups = "drop")
}

# Call function
nounphrases_tbl <- process_nounphrases(np_df)

# Attach enriched info to combined events for downstream personas / visualizations
all_events_enriched <- all_events %>%
  left_join(entities_tbl, join_by("event_id" == "doc_id")) %>%
  left_join(nounphrases_tbl, join_by("event_id" == "doc_id"))


```

Having extracted named entities and noun phrases, we can enrich our current events table with this additional information.

## 3.3. Natural Language Processing (NLP)

The importance of any given event is subjective to the wants and needs of our end user. When people actually read detailed descriptions they typically converge on a heuristic response, given a mixture of multiple different factors, to decide whether or not it is relevant. To mimic this, we must define personas, and create lexicons, to measurably compare events to our end users preferences.

### Why and How to use NLP?

-   *Generating a precise understanding of the topic of the event.*

-   *Differentiate topics among equally relevant events.*

-   *Defining criteria by which we can choose relevant events.*

-   *Visualizing distributions of event relevance, and rationalizing top 3 choices from it.*

We begin by tokenizing each word in all the event descriptions and plotting their overall use to extrapolate information about trending event topics.

```{r}

nlp_tokens <- all_events_raw %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description, token = "ngrams", n = 2)

nlp_token_1 <- all_events_raw %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description)%>%
  dplyr::anti_join(tidytext::stop_words)



# We need to remove stop words, I can do so by creating a new column per words, replacing stop words, and seeing what happens



nlp_tokens <- nlp_tokens %>% 
  tidyr::separate_wider_delim(word, delim = " ", names = c("item1",
                                                           "item2"
                                                           ))

# for loop to remove stop_words
for(i in 2:nrow(nlp_tokens)){
  for(j in 2:length(nlp_tokens)){
    
    if(nlp_tokens[i, j] %in% tidytext::stop_words$word){
      
      nlp_tokens[i,j] <- ""
      
    }else{
      nlp_tokens[i,j] <- nlp_tokens[i,j]
    }
  }
}

nlp_tokens <- nlp_tokens %>% 
  tidyr::unite(item1, 2:length(nlp_tokens),  sep = " ")%>%
  dplyr::transmute(event_id, phrase = item1)

nlp_tokens$phrase <- nlp_tokens$phrase %>% gsub("^\\s+|\\s+$", "", .)

nlp_tokens <- nlp_tokens %>% dplyr::filter(stri_count_words(phrase) >= 2) %>% unique()


rbind(

(nlp_tokens%>%
  count(phrase, name = "count")%>%
   dplyr::rename("token" = "phrase")%>%dplyr::mutate(type = "Phrase")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n = 15)),
(nlp_token_1%>%
   count(word, name = "count")%>%
   dplyr::rename("token" = "word")%>%dplyr::mutate(type = "word")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n=15))

)%>%
  ggplot2::ggplot(aes(x = reorder(token, count), y = count, fill = token))+
  geom_col()+
  theme(legend.position = "none",
        panel.grid = element_line(colour = "lightgrey", linewidth = 0.2),
        panel.background = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())+
  coord_flip()+
  facet_grid(.~type)+
  labs(x = "Tokens", y = "Mentions")
```

We note that there are multiple phrases that are potentially counted repeatedly within the same context. This means that two phrases are different by only one word, and because of that, each word is shown twice in our list of phrase tokens. This happens how?

Suppose we are splitting a text into bi-grams (tokenizing each pair of words). The text is :

"The principle is calling us!"

We will get the following word pairs.

-   "The principle"

-   "principle is"

-   "is calling"

-   "calling us!"

Analyzing these sorts of bi-grams can yield misleading results without refinement.

We also face a problem of non-descriptive phrases containing filler words not already stopped out. Differentiating contexts for n-grams \>1 can prove quite difficult.

Why this matters?:

To side-step both these problems, and enhance our analysis, we can rely on our enriched data frame to pull noun-phrases to evaluate trends. This will limit our n-gram analysis to only noun phrases.

```{r}

np_clean <- np_df %>% 
  unnest_tokens(output = word, input = text) %>% 
  anti_join(stop_words, by = "word") %>%
  group_by(doc_id, root_id, start_id, length) %>%  # keys for a phrase
  summarise(clean_text = str_c(word, collapse = " "),
              length = n(),
              .groups = "drop") %>%
  filter(clean_text != "") %>% 
  filter(length > 1) %>%
  mutate(clean_text = str_squish(clean_text))

rbind(
(np_clean%>%
  count(clean_text, name = "count")%>%
   dplyr::rename("token" = "clean_text")%>%dplyr::mutate(type = "Noun_Phrase")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n = 15)),
(nlp_token_1%>%
   count(word, name = "count")%>%
   dplyr::rename("token" = "word")%>%dplyr::mutate(type = "word")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n=15))

)%>%
  ggplot2::ggplot(aes(x = reorder(token, count), y = count, fill = token))+
  geom_col()+
  theme(legend.position = "none",
        panel.grid = element_line(colour = "lightgrey", linewidth = 0.2),
        panel.background = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())+
  coord_flip()+
  facet_grid(.~type)+
  labs(x = "Tokens", y = "Mentions")

```

At a high level, the information provided by the above graphics can inform the end user on trending topics in their area. What it does not do is materially move our analysis closer to pinpointing three relevant events. In order to make real headwinds, we cannot divorce our statistics from group level assignment.

A different high level view of relevant phrases is provided by the below graph:

```{r}

np_igraph <- np_clean%>%
  dplyr::select(doc_id, clean_text)%>%
  igraph::graph_from_data_frame()
  
V(np_igraph)$type <- bipartite_mapping(np_igraph)$type
  
np_igraph %>%  
  ggraph(layout = "nicely")+
  geom_edge_link(show.legend = FALSE)+
  geom_node_point(aes(colour = type), size = 5)+
  geom_node_text(aes(label = name),
                 size = 4,
                 vjust = 1.3,
                 hjust = 0.5)+
  theme(legend.position = "none",
        panel.background = element_blank(),
        panel.border = element_rect(colour = "black"))

```

Given the above graphic, you can extract multiple key takeaways about the event landscape you are a part of. High levels of fragmentation would signify an overall lack of synchronicity among events. A more interconnected cobweb of key phrases and events would support the belief that there is a dominant trend.

We can observe some clustering of events allowing us to establish some inter-connectivity if any. We can further refine our uni-gram and noun phrase matching to include only events with relevant words. We utilize lexicons such as the one below as a comparative list.

```{r}
networking_words <- list(
  
  keywords = c("network","networking", "meet", "job", "work","friend",
               "relationship", "mixer","peer"),
  
  others = c("engage", "exchange","social", "mingle", "introduction", 
             "collaborate", "partnership", "discuss", "roundtable")
  
)

```

```{r}
persona_finance_student <- list(
  
  keywords = c(
    "finance", "financial", "investment", "investing", "markets",
    "economics", "trading", "derivatives", "valuation", "analytics",
    "data", "economy", "portfolio", "risk", "venture", "credit", "debt",
    "banking", "capital", "wealth", "equity", "fund", "money"
  ),
  
  priority_keywords = c("finance", "internship", "trading", "analyst", "mentorship", "professional"),
  
  locations = c("virtual", "online"),
  
  event_types = c("networking", "career fair", "seminar")
)


data.frame(Keywords = paste(persona_finance_student$keywords, collapse = ", "), Priority_Keys = paste(persona_finance_student$priority_keywords, collapse = ", "), Locations = paste(persona_finance_student$locations, collapse = ", "))%>%
  gt::gt()

```

```{r}
persona_tech_student <- list(
  
  keywords = c(
    "tech", "technology", "software", "developer", "coding",
    "programming", "data", "python", "cloud", "app",
    "machine", "learning", "ai", "intelligence", "artificial"
  ),
  
  priority_keywords = c(
    "hackathon", "developer", "internship", "programming"
  ),
  
  locations = c("calgary"),
  
  event_types = c("workshop", "webinar", "hackathon")
)

```

```{r}
persona_entrepreneur <- list(
  
  keywords = c(
    "entrepreneur", "startup", "business", "innovation",
    "pitch", "founder", "funding", "capital", "growth", "strategy",
    "marketing", "strategy", "leadership", "sales", "money", "sales"
  ),
  
  priority_keywords = c(
    "sales", "pitch", "startup", "entrepreneur", "mentor"
  ),
  
  locations = c("virtual", "online"),
  
  event_types = c(
    "networking", "pitch", "seminar"
  )
)
```

### 3.1. Persona and Lexicon Matching

To score events by relevance, we match keywords in pre-defined lexicons containing preferred language, with each event's description. Below is a visual example of the first step in our machine learning workflow.

```{r}


topic <- persona_finance_student$keywords # choose your topic of interest here

nlp_token_1$Contains <- "" # initializing the contains column 


for(i in 1:nrow(nlp_token_1)){

nlp_token_1$Contains[i] <- grepl(nlp_token_1[i,2], paste(topic, collapse = ""))

}

nlp_fit <- nlp_token_1 %>%
  dplyr::filter(Contains == "TRUE")%>%
  dplyr::group_by(event_id)%>%
  dplyr::count(word, name = "Matches")%>%
  dplyr::arrange(desc(Matches))

nlp_fit %>%
  dplyr::group_by(event_id)%>% dplyr::summarise(Matches = sum(Matches))%>%
  dplyr::arrange(desc(Matches))%>% dplyr::slice_head(n = 10)%>%
  ggplot2::ggplot(aes(x = Matches, y = reorder(event_id, Matches), fill = Matches))+
  geom_col()+
  theme(panel.background = element_blank(),
        panel.grid = element_line(colour = "lightgrey"),
        legend.position = "none")+
  labs(x = "Matches", y = "Event ID",
       title = "Number of Persona Matches by Event")




```

At its core, we count matches between defined criteria and event descriptions. In the above example, taking count as score, the top ranking event will thus be recommended to our end user. This is a simplified example.

```{r}

# Attempting same logic as matches first, **no use for lexicon dataframe
#Initializing dataframe (this is not AI btw)

nlp_type <- nlp_token_1
nlp_type$Networking <- ""



for(i in 1:nrow(nlp_type)){

nlp_type$Networking[i] <- grepl(nlp_type[i,2], paste(networking_words, collapse = ""))


}

nlp_type <- nlp_type %>%
  dplyr::mutate(across(3:ncol(nlp_type), ~ as.integer(as.logical(.))))


nlp_type %>%
  tidyr::pivot_longer(cols = c(4:ncol(nlp_type)),
                      names_to = "Type",
                      values_to = "Score")%>%
  dplyr::filter(Score != 0)%>%
  ggplot2::ggplot(aes(x = reorder(as.character(event_id), Score), 
                      y = Score,
                      fill = Score))+
  geom_col()+
  labs(title = "Event Type Scores",
       x = "Event ID")+
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.background = element_blank(),
        legend.position = "none",
        panel.grid.major.y = element_line(colour = "lightgrey"))+
  coord_flip()
  


# Not generalized so far
nlp_type_scores <- nlp_type%>%
  dplyr::group_by(event_id)%>%
  dplyr::summarise(Network_Score = sum(Networking),
                   Matches = sum(Contains))%>%
  dplyr::filter(Matches == 1)



```

*Shown above is another example of key word matching, this time however, we are calculating a sentiment score based on the event's inclination towards the use of networking words.*

### 3.2. N-gram Matching

Expanding from the uni-gram analysis, phrases can provide extra relevance, especially if repeated within or across events. Below are network graphs with information we will combine and use to assess relevance for each event.

```{r}


persona1 <- persona_tech_student


  
np_clean$Contains = ""
  
for(i in 1:nrow(np_clean)){  

  for(j in 1:length(persona1$keywords)){
    
      np_clean$Contains[i] <-grepl(persona1$keywords[j], 
                                    np_clean$clean_text[i],
                                    ignore.case = TRUE)
  
      if(np_clean$Contains[i] == TRUE){
        
        break
        
      } else{
        
        next
        
      }
  }
}




np_graph_hold <- np_clean  %>% dplyr::filter(Contains != "FALSE")%>%
  dplyr::select(doc_id, clean_text)%>%
  igraph::graph_from_data_frame()

V(np_graph_hold)$type <- bipartite_mapping(np_graph_hold)$type

np_graph_hold%>%
  ggraph(layout = "fr") +
  geom_edge_link(show.legend = FALSE) +
  geom_node_point(aes(color = type), size = 5) +
  geom_node_text(aes(label = name),
                 size = 3,
                 vjust = 1.5,
                 hjust = .4) +
  theme(panel.background = element_blank(),
        legend.position = "none")+
  labs(title = "Phrases in Relevant Events")




np_holder <- np_clean %>%
  dplyr::filter(Contains != "FALSE")


```

```{r}

np_pairwise <- np_clean %>%
  dplyr::filter(doc_id %in% np_holder$doc_id)%>%
  dplyr::select(doc_id, clean_text)%>%
  widyr::pairwise_cor(
    item = clean_text,
    feature = doc_id,
    sort = T,
    upper = FALSE)%>%
  dplyr::filter(correlation > 0)


np_pairwise%>%
  igraph::graph_from_data_frame() %>%
  ggraph(layout = "grid") +
  geom_edge_link(show.legend = FALSE) +
  geom_node_point(color = "orange", size = 5) +
  geom_node_text(aes(label = name),
                 size = 3,
                 vjust = 1.5,
                 hjust = .5) +
  theme_void()+
  labs(title = "Correlated Phrases in Relevant Events")



```

The First - "Phrases in Relevant Events" - is a network graph tying event_id with all known noun phrases that include relevant information. This will not directly be used to calculate relevant score because keyword matching is already incorporated through uni-gram matching.

Each individual edge node does not reflect significance by virtue of existing. An exception is phrase nodes connecting to 1 or more event. If a noun-phrase containing a key word is mentioned in more than one event, we consider it a "Trendy" word. This can aid our analysis if we can isolate a trendiness score per event.

The Second - "Correlated Phrases in Relevant Events" - is a pairwise correlation between noun-phrases, not grouped by event. While this provides useful information on what words are used together, and which are not on a large scale, we use it to calculate trendiness.

In simple terms, our pairwise output links all different phrases used together across event descriptions. We use this to analyze the number of pairings each event has generated via its noun phrases.

[Defining Trendiness:]{.underline}

Given a count for noun phrases per event, and pair links generated per event, we calculate trendiness as:

(# of Pairs) / ^(# of noun phrases)^C~2~

We calculate trendiness as such because pairwise analysis provides pairings for noun phrases within the same event, potentially leading to incorrect interpretations.

If the numerator (# of Pairs) is larger than the possible combinations of noun phrase pairs per event, we can conclude that one or more noun phrases in that event is paired with a different noun phrase else where.

An output of the scoring is found below:

```{r}

# Trendiness Score

np_clean_ids <- np_clean %>%
  dplyr::filter(doc_id %in% np_holder$doc_id)

# ^ Grabs all noun phrases for relevant events

noun_count <- np_clean_ids%>%
  dplyr::group_by(doc_id)%>%
  count(doc_id,
        name = "count_nouns")

np_pairs <- np_pairwise %>% count(item1, name = "Pairs")


np_pairs <- np_pairs %>%
  left_join(np_clean_ids %>% select(clean_text, doc_id),
          by = c("item1" = "clean_text"))



np_pairs$doc_id <- as.integer(np_pairs$doc_id)

np_pairs %>%
  dplyr::select(-item1)%>%
  dplyr::group_by(doc_id)%>%
  dplyr::summarise(Pairs = sum(Pairs))%>%
  dplyr::left_join(noun_count)%>%
  dplyr::mutate(Trendiness = Pairs/
                  pmax(choose(count_nouns, 2), 1))%>%
  drop_na()%>%
  ggplot2::ggplot(aes(x = reorder(as.character(doc_id), Trendiness),
                      y = Trendiness,
                      fill = as.character(doc_id)))+
  geom_col()+
  labs(title = "Trendiness Meter Per Event",
       x = "Event IDs")+
  theme(panel.background = element_blank(),
        legend.position = "none")




```

*In a highly idiosyncratic event landscape, you can expect a default trendiness score of 1 across the board.*

# 4. Machine Learning Workflow

```{r, include = FALSE}
# attempt at making ML workflow chart

workflow_graph <- data.frame(Workflow = c("Criteria Matching",
                                       "Building Score Tables",
                                       "Calculating Relevant Score",
                                       "Identifying Top Events",
                                       "Collecting Information"),
                       Order = c(1:5),
                       y_ax = c(2,2,2,2,2))

workflow_graph %>%
  ggplot2::ggplot(aes(x = Order, 
                      y = y_ax, fill = Workflow,
                      label = Workflow))+
  geom_point(size = 3,
             colour = "black")+
  theme(panel.background = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.line.x = element_blank(),
        axis.line.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        legend.position = "none")
```

We synthesize the data using a series of functions to output data for three alternative personas. The functions in our ML workflow are operable with raw data, lists, and lightly modified data frames as arguments. The workflow works as such:

1.  Criteria matching.
2.  Building score tables.
3.  Aggregating into original data frame.
4.  Identifying top events.
5.  Collecting Information.

We explore more in our results section.

```{r}
desc_tokens <- all_events %>%
  select(event_id, description) %>%
  filter(!is.na(description)) %>%
  tidytext::unnest_tokens(output = word, input = description) %>%
  anti_join(tidytext::stop_words)

title_tokens <- all_events %>%
  select(event_id, title) %>%
  filter(!is.na(title)) %>%
  tidytext::unnest_tokens(output = word, input = title) %>%
  anti_join(tidytext::stop_words, by = "word")

```

```{r}
# DONE MATCH DESC SYNC WITH NLP
match_desc <- function(tokens, persona) {

  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }
# counting each instance per event that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Desc_Matches")
  
}

#__________________________________________________________________________________
# DONE MATCH TITLE SYNC WITH NLP
match_title <- function(tokens, persona) {
  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }

# counting each instance per event title that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Title_Matches")
}

#__________________________________________________________________________________

event_sentiment_key <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$keywords, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Key_Matches")


}

#__________________________________________________________________________________

event_sentiment_other <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$others, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Other_Matches")

}

#__________________________________________________________________________________


match_location <- function(events, persona) {
  events %>%
    mutate(
      location_lower = tolower(location),
      Location_Matches = if_else(
        str_detect(location_lower, str_c(persona$locations, collapse = "|")),
        1, 0, missing = 0)) %>%
    select(event_id, Location_Matches)
}


match_type <- function(events, persona) {
  events %>%
    mutate(
      desc_lower = tolower(description),
      Type_Matches = if_else(
        str_detect(desc_lower, str_c(persona$event_types, collapse = "|")),
        1L, 0L, missing = 0L)) %>%
    select(event_id, Type_Matches)
}

#__________________________________________________________________________________

match_trendy <- function(np_clean){ #slightly fragile

np_holder <- np_clean %>%
  dplyr::filter(Contains != "FALSE")
  
  
np_pairwise <- np_clean %>%
  dplyr::filter(doc_id %in% np_holder$doc_id)%>%
  dplyr::select(doc_id, clean_text)%>%
  widyr::pairwise_cor(
    item = clean_text,
    feature = doc_id,
    sort = T,
    upper = FALSE)%>%
  dplyr::filter(correlation > 0)

# Complete repeat of the visualization above ^^^
np_clean_ids <- np_clean %>%
  dplyr::filter(doc_id %in% np_holder$doc_id)

# ^ Grabs all noun phrases for relevant events

noun_count <- np_clean_ids%>%
  dplyr::group_by(doc_id)%>%
  count(doc_id,
        name = "count_nouns")

np_pairs <- np_pairwise %>% count(item1, name = "Pairs")


np_pairs <- np_pairs %>%
  left_join(np_clean_ids %>% select(clean_text, doc_id),
          by = c("item1" = "clean_text"))



np_pairs$doc_id <- as.integer(np_pairs$doc_id)

np_pairs %>%
  dplyr::select(-item1)%>%
  dplyr::group_by(doc_id)%>%
  dplyr::summarise(Pairs = sum(Pairs))%>%
  dplyr::left_join(noun_count)%>%
  dplyr::mutate(Trendiness = Pairs/
                  pmax(choose(count_nouns, 2), 1))%>%
  drop_na()%>%
  dplyr::transmute(event_id = doc_id, Trendiness)


}
#__________________________________________________________________________________







match_highly_relevant <- function(){
 
}






```

```{r}
build_event_features <- function(all_events, desc_tokens, title_tokens, 
                                 persona, networking_words, np_clean) {
  
  desc_hits_tbl     <- match_desc(desc_tokens, persona)
  title_hits_tbl    <- match_title(title_tokens, persona)
  sentiment_key_tbl <- event_sentiment_key(desc_tokens, networking_words)
  sentiment_other_tbl <- event_sentiment_other(desc_tokens, networking_words)
  loc_match_tbl     <- match_location(all_events, persona)
  type_match_tbl    <- match_type(all_events, persona)
  trendy_tbl        <- match_trendy(np_clean)
  
  all_events %>%
    left_join(desc_hits_tbl,  by = "event_id") %>%
    left_join(title_hits_tbl, by = "event_id") %>%
    left_join(sentiment_key_tbl, by = "event_id") %>%
    left_join(sentiment_other_tbl, by = "event_id") %>%
    left_join(loc_match_tbl,  by = "event_id") %>%
    left_join(type_match_tbl, by = "event_id") %>%
    left_join(trendy_tbl,     by = "event_id")%>%
    mutate(
      Desc_Matches          = replace_na(Desc_Matches, 0),
      Title_Matches         = replace_na(Title_Matches, 0),
      Sentiment_Key_Matches       = replace_na(Sentiment_Key_Matches, 0),
      Sentiment_Other_Matches       = replace_na(Sentiment_Other_Matches, 0),
      Location_Matches      = replace_na(Location_Matches, 0),
      Type_Matches          = replace_na(Type_Matches, 0),
      Trendiness            = replace_na(Trendiness, 0)
    )
}
```

```{r}
score_event_relevance <- function(events_features) {
  
  scored <- events_features %>%
    mutate(
      raw_score =
        1.0 * Desc_Matches +
        1.5 * Title_Matches +
        1.5 * Sentiment_Key_Matches +
        0.2 * Sentiment_Other_Matches +
        1.0 * Location_Matches +
        1.0 * Type_Matches +
        Trendiness
    )
  
  min_val <- min(scored$raw_score, na.rm = TRUE)
  max_val <- max(scored$raw_score, na.rm = TRUE)
  
  scored %>%
    mutate(relevance_score = round(1 + 9 * ((raw_score - min_val) / (max_val - min_val)), 2)) %>%
    
    arrange(desc(relevance_score))
}

```

```{r}
get_top_events <- function(events_scored, n = 3) {
  events_scored %>%
    slice_head(n = n) %>%
    select(
      title,
      date,
      start_time,
      end_time,
      location,
      description,
      event_url,
      relevance_score
    )
}
```

# 5. Results

```{r}
events_features_1 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_finance_student,
  networking_words,
  np_clean
)

events_scored_1 <- score_event_relevance(events_features_1)
top3_events_1 <- get_top_events(events_scored_1, 3)
top3_events_1

```

```{r}
events_features_2 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_tech_student,
  networking_words,
  np_clean
)

events_scored_2 <- score_event_relevance(events_features_2)
top3_events_2 <- get_top_events(events_scored_2, 3)
top3_events_2
```

```{r}
events_features_3 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_entrepreneur,
  networking_words,
  np_clean
)

events_scored_3 <- score_event_relevance(events_features_3)
top3_events_3 <- get_top_events(events_scored_3, 3)
top3_events_3
```

# 6. Conclusion
