---
title: "Event Scraper for In-Person Networking"
author: "Bao Nguyen, Haitam Aksikas, Ayman Arman"
format:
  html:
    embed-resources: true
---

```{r, include = FALSE}

knitr::opts_chunk$set(echo = FALSE,
                  warning = FALSE,
                  message = FALSE,
                  fig.width = 4,
                  fig.height = 3,
                  fig.align = "center",
                  out.width = "75%")
```

```{r}
library(purrr) 
library(tibble) 
library(tidyverse) 
library(spacyr) 
library(textdata) 
library(jsonlite) 
library(tidytext)
library(igraph)
library(ggraph)
library(gt)
library(stringi)
library(ggplot2)
```

# 1. Introduction

## 1.1. The Current Job Market

Job seekers are focusing more on securing internships or new graduate roles through strong connections and sponsors within their field. Young professionals are increasingly reliant on open invitation networking events to connect with individuals and begin forming a robust network around them Event organizers typically market such events across a multitude of websites, each usually focusing on specific geographical areas, or overall topic groups. The volume of events can be somewhat daunting, leaving time constrained young professionals hearing about useful events only after they happen.

## 1.2. Our Solution

We plan to limit the amount of time searching for events by scraping major websites containing event ads, and displaying the most relevant events to our end user. Utilizing natural language processing techniques, we will score events based on relevance, sentiment, phrases used, and other key matches. Using this program, we aim to provide:

-   The top three relevant events given an overall relevance score

-   Names of people and companies to get in touch with

-   Details to sign up for the above events

## 1.3. Professional Personas

We will have the following personas as examples to demonstrate how events are relevant to each individual:

-   Tech persona: interested in networking as well as more technical workshops and hackathons

-   Finance persona: interested in events related to networking, speaker series, and case competitions

-   Entrepreneurship persona: interested in a wide variety of business, pitching and startup events

These personas provide broad coverage for areas that many young professionals are looking to enter today, with finance and entrepreneurship having some overlap in interest.

# 2. Data Sources

The following websites are used to find and extract events for further consideration:

-   Eventbrite: a popular website for companies and organizations to host live events in many niches

-   Platform Calgary: a Calgary-based technology organization that hosts many local tech and business events

-   Edmonton Regional Innovation Network (ERIN): a coalition of organizations in the Edmonton region that support entrepreneurship and business activity

Eventbrite contains a variety of available data, while Platform Calgary and ERIN represent specific locales and niches to compare against.

# 3. Methodology

## 3.1. Web Scraping

To gather data, JavaScript files are written to scrape information from the events section of their respective websites. Each website's unique DOM is accessed to extract event titles, locations, dates and times as applicable. From there, individual pages under the events section are accessed to extract full event descriptions and links. Lastly, initial preprocessing is performed to extract the day of week.

In the end, all upcoming events from ERIN and Platform Calgary are obtained, while all business events for the week ahead from Eventbrite are obtained. This process is flexible to run at any time period, and JSON outputs from each JavaScript file follow this data schema:

-   Platform (platform): what website the event appeared in

<!-- -->

-   Title (title): the event's full title

<!-- -->

-   Link (event_url): link to view and register in the event

<!-- -->

-   Date (date): event date in mm/dd/yyyy format

<!-- -->

-   Day of week (day_of_week)

<!-- -->

-   Start time (start_time): when the event starts

-   End time (end_time): when the event ends

-   Location (location): where the event takes place

-   Description (description): the full event description

```{r}
edmontonrin <- system("node ./edmontonrin.js", intern = TRUE) %>%
  fromJSON() %>%
  as_tibble()
```

```{r}
platformcalgary <- system("node ./platformcalgary.js", intern = TRUE) %>%
  fromJSON() %>% 
  as_tibble()
```

```{r,include = FALSE}
eventbrite <- system("node ./eventbrite.js", intern = TRUE) %>%
  fromJSON() %>%
  as_tibble()
```

Combining all scraped data together into a full table and labeling each extracted event, we are ready to analyze further.

## 3.2. Named Entity Recognition

Named entity recognition is an important process in natural language processing to identify special tokens such as people, organizations, places, and topics. These specific entities represent potential connections, companies, and topics that are expected to appear, directly leading to informed decision-making for what events to attend. The spaCy package will be crucial for extracting such entities after preprocessing raw event descriptions to better parse extracted tokens.

```{r}
all_events_raw <- bind_rows(edmontonrin, platformcalgary, eventbrite) %>% 
  dplyr::mutate(event_id = row_number()) %>%
  dplyr::select(event_id, dplyr::everything())

# Preprocess description for spaCy
all_events <- all_events_raw %>%
  mutate(
    description_clean = description %>%
      str_replace_all("([a-z])(\\.[A-Z])", "\\1. \\2") %>% # add space after a period glued to a capital
      str_replace_all("([a-z])(,[A-Z])", "\\1, \\2") %>% # add space after a comma glued to a capital
      str_replace_all("([A-Za-z])'s", "\\1 's") %>% # ensure space before possessive ’s
      str_replace_all("([A-Z]{2,})([A-Z][a-z])", "\\1 \\2") %>%  # split run-on capitals
      str_squish()
  )
```

Note: potentially include an example raw vs preprocessed description here to illustrate differences

We install and use the transformer model in spaCy to obtain the most accurate results possible. For people, organizations, and places, special entities are extracted and cleaned separately. For topics, applicable noun phrases that can potentially capture event relevance are extracted as various length n-grams.

```{r}
# Install and prepare spaCy transformer model
spacy_install()
spacy_download_langmodel("en_core_web_trf")
```

```{r}
# Initialize spaCy
spacy_initialize(model = "en_core_web_trf")

# NER extraction across all descriptions
ner_df <- spacy_extract_entity(
  all_events$description_clean,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% # Preprocessing dataframe before further cleaning
  tibble::as_tibble() %>% 
  filter(!str_detect(text, "@"), # Remove email addresses
         !str_detect(text, "https?://")) # Remove links

# Noun phrase extraction across all descriptions
np_df <- spacy_extract_nounphrases(
  all_events$description_clean,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% # Preprocessing dataframe before further cleaning
  tibble::as_tibble()

spacy_finalize()
```

```{r}
# Count unique entities by type
ner_counts <- ner_df %>%
  distinct(doc_id, text, ent_type) %>%   # Deduplicate entities
  count(ent_type, sort = TRUE)

# Entity frequency of extracted events
ggplot(ner_counts, aes(x = reorder(ent_type, n), y = n)) +
  geom_col(fill = "blue") +
  geom_text(aes(label = scales::comma(n)), hjust = -0.1, color = "black") +
  coord_flip() +
  labs(
    title = "Entity Types Found in Event Descriptions",
    x = "Entity Type",
    y = "Token Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold")
  )

```

Note: make a potential bar chart for showing how many tokens of each entity type show up -\> reasoning for only retaining PERSON, ORG, GPE

Out of all possible types, spaCy represents our entities of interest (people, organizations, and places) as PERSON, ORG, and GPE. We develop a process to improve extraction quality by removing stop words and duplicate entities as well as restoring capitalization for organization acronyms and names to retain proper cases. Lastly, we indicate null for each event without some of these entities, which will be useful for later ranking event relevance in terms of details present.

```{r}
# Keep useful entity types and regroup by category
keep_types <- c("PERSON", "ORG", "GPE")
entity_category_map <- tibble(
  entity_type = keep_types,
  category = c("people", "organizations", "places")
)

# Remove stop words and prepare clean entities for each event
process_entities <- function(ner_df) {
  ner_df %>%
    filter(ent_type %in% keep_types) %>%
    mutate(
      entity_id   = row_number(), # Label each event
      orig_entity = text # Save original text
    ) %>%
    unnest_tokens(token, text) %>%
    anti_join(stop_words, by = c("token" = "word")) %>%
    group_by(doc_id, ent_type, entity_id, orig_entity) %>%
    summarise(
      entity_clean_tokens = list(token), # Get list of clean entities for each event
      .groups = "drop"
    ) %>%
    mutate(
      entity_clean = map_chr(entity_clean_tokens, ~ str_c(.x, collapse = " ")),
      entity_clean = str_squish(entity_clean),
      entity_clean = if_else(
        str_detect(orig_entity, "[A-Z]{2,}") | str_detect(orig_entity, "[A-Za-z][A-Z]"),
        orig_entity, # keep acronyms/mixed case
        str_to_title(entity_clean) # normalize regular names
      )
    ) %>%
    filter(entity_clean != "") %>%
    distinct(doc_id, ent_type, entity_clean) %>%
    left_join(entity_category_map, by = c("ent_type" = "entity_type")) %>%
    group_by(doc_id, category) %>%
    summarise(
      entities = list(unique(entity_clean)), # deduplicated vectors
      .groups = "drop"
    ) %>% 
    pivot_wider(names_from = category, values_from = entities) %>% 
    mutate(across(c(people, organizations, places),
                  ~ map(.x, ~ if (length(.x) == 0) NA else .x)))
}

# Call function
entities_tbl <- process_entities(ner_df)
```

Similarly, we also clean noun phrases by removing stop words where necessary and recombining tokens based on event. Here, we remeasure each recombined entity's length to remove unigrams as they are not indicative of potential discussed topics.

```{r}
# Remove stop words from extracted noun phrases
process_nounphrases <- function(np_df) {
  np_df %>% 
    unnest_tokens(output = word, input = text) %>% 
    anti_join(stop_words, by = "word") %>%
    group_by(doc_id, root_id, start_id, length) %>%  # Keys for a phrase
    summarise(clean_text = str_c(word, collapse = " "),
              length = n(),
              .groups = "drop") %>%
    filter(clean_text != "") %>% 
    filter(length > 1) %>% # Remove unigrams
    mutate(clean_text = str_squish(clean_text)) %>%
    group_by(doc_id) %>%
    summarise(noun_phrases = list(unique(clean_text)), .groups = "drop")
}

# Call function
nounphrases_tbl <- process_nounphrases(np_df)
```

Having extracted named entities and noun phrases, we can enrich our current events table with this additional information. The chart below shows how

```{r}
# Attach enriched info to combined events 
all_events_enriched <- all_events %>%
  left_join(entities_tbl, join_by("event_id" == "doc_id")) %>%
  left_join(nounphrases_tbl, join_by("event_id" == "doc_id")) %>% 
  mutate(
    across(
      where(is.list),
      ~ map(.x, ~ if (is.null(.x)) NA else .x)
    )
  )
```

```{r}
# Convert nulls and NAs to FALSE
has_values <- function(x) {
  if (is.null(x) || (length(x) == 1 && all(is.na(x)))) {
    FALSE
  } else {
    length(x) > 0
  }
}

# Get entity coverage for each event
info_status_df <- all_events_enriched %>%
  mutate(
    has_people = map_lgl(people,        has_values),
    has_orgs   = map_lgl(organizations, has_values),
    has_places = map_lgl(places,        has_values),
    info_status = case_when(
      has_people & has_orgs & has_places             ~ "Full Information",
      has_people & has_orgs & !has_places            ~ "Missing Locations",
      has_people & !has_orgs & has_places            ~ "Missing Organizations",
      !has_people & has_orgs & has_places            ~ "Missing People",
      has_people & !has_orgs & !has_places           ~ "Only People",
      !has_people & has_orgs & !has_places           ~ "Only Organizations",
      !has_people & !has_orgs & has_places           ~ "Only Locations",
      TRUE                                           ~ "Missing All"
    ) %>%
      factor(levels = c("Full Information",
                        "Missing People",
                        "Missing Organizations",
                        "Missing Locations",
                        "Only People",
                        "Only Organizations",
                        "Only Locations",
                        "Missing All"))
  )

# Count entities of the same type
status_counts <- info_status_df %>%
  count(info_status, name = "events")

# Graph entity coverage
ggplot(status_counts, aes(x = info_status, y = events)) +
  geom_col(fill = "blue") +
  geom_text(aes(label = events), hjust = -0.3, color = "black", size = 3.5) +
  labs(
    title = "Entity Coverage Across Events",
    x = NULL,
    y = "Number of Events"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.text.x = element_text(size = 9),
    panel.grid.major.x = element_blank()
  ) +
  coord_flip()
```

## 3.3. Natural Language Processing (NLP)

The importance of any given event is subjective to the wants and needs of our end user. When people actually read detailed descriptions they typically converge on a heuristic response, given a mixture of multiple different factors, to decide whether or not it is relevant. To mimic this, we must define personas, and create lexicons, to measurably compare events to our end users preferences.

### 3.3.1. Why and How to use NLP?

-   Generating a precise understanding of the topic of the event.

-   Differentiate topics among equally relevant events.

-   Defining criteria by which we can choose relevant events.

-   Visualizing distributions of event relevance, and rationalizing top 3 choices from it.

We begin by tokenizing each word in all the event descriptions and plotting their overall use to extrapolate information about trending event topics.

```{r}

nlp_tokens <- all_events_raw %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description, token = "ngrams", n = 2)

nlp_token_1 <- all_events_raw %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description)%>%
  dplyr::anti_join(tidytext::stop_words)



# We need to remove stop words, I can do so by creating a new column per words, replacing stop words, and seeing what happens



nlp_tokens <- nlp_tokens %>% 
  tidyr::separate_wider_delim(word, delim = " ", names = c("item1",
                                                           "item2"
                                                           ))

# for loop to remove stop_words
for(i in 2:nrow(nlp_tokens)){
  for(j in 2:length(nlp_tokens)){
    
    if(nlp_tokens[i, j] %in% tidytext::stop_words$word){
      
      nlp_tokens[i,j] <- ""
      
    }else{
      nlp_tokens[i,j] <- nlp_tokens[i,j]
    }
  }
}

nlp_tokens <- nlp_tokens %>% 
  tidyr::unite(item1, 2:length(nlp_tokens),  sep = " ")%>%
  dplyr::transmute(event_id, phrase = item1)

nlp_tokens$phrase <- nlp_tokens$phrase %>% gsub("^\\s+|\\s+$", "", .)

nlp_tokens <- nlp_tokens %>% dplyr::filter(stri_count_words(phrase) >= 2) %>% unique()


rbind(

(nlp_tokens%>%
  count(phrase, name = "count")%>%
   dplyr::rename("token" = "phrase")%>%dplyr::mutate(type = "Phrase")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n = 15)),
(nlp_token_1%>%
   count(word, name = "count")%>%
   dplyr::rename("token" = "word")%>%dplyr::mutate(type = "word")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n=15))

)%>%
  ggplot2::ggplot(aes(x = reorder(token, count), y = count, fill = token))+
  geom_col()+
  theme(legend.position = "none",
        panel.grid = element_line(colour = "lightgrey", linewidth = 0.2),
        panel.background = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())+
  coord_flip()+
  facet_grid(.~type)+
  labs(x = "Tokens", y = "Mentions")
```

We note that there are multiple phrases that are potentially counted repeatedly within the same context. This means that two phrases are different by only one word, and because of that, each word is shown twice in our list of phrase tokens. To enhance our analysis, we can rely on our enriched data frame to pull noun-phrases to evaluate trends. This will limit our n-gram analysis to only noun phrases.

```{r}

np_clean <- np_df %>% 
  unnest_tokens(output = word, input = text) %>% 
  anti_join(stop_words, by = "word") %>%
  group_by(doc_id, root_id, start_id, length) %>%  # keys for a phrase
  summarise(clean_text = str_c(word, collapse = " "),
              length = n(),
              .groups = "drop") %>%
  filter(clean_text != "") %>% 
  filter(length > 1) %>%
  mutate(clean_text = str_squish(clean_text))

rbind(
(np_clean%>%
  count(clean_text, name = "count")%>%
   dplyr::rename("token" = "clean_text")%>%dplyr::mutate(type = "Noun_Phrase")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n = 15)),
(nlp_token_1%>%
   count(word, name = "count")%>%
   dplyr::rename("token" = "word")%>%dplyr::mutate(type = "word")%>%
   dplyr::arrange(desc(.))%>% dplyr::slice_head(n=15))

)%>%
  ggplot2::ggplot(aes(x = reorder(token, count), y = count, fill = token))+
  geom_col()+
  theme(legend.position = "none",
        panel.grid = element_line(colour = "lightgrey", linewidth = 0.2),
        panel.background = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())+
  coord_flip()+
  facet_grid(.~type)+
  labs(x = "Tokens", y = "Mentions")

```

At a high level, the information provided by the above graphics can inform the end user on trending topics in their area. What it does not do is materially move our analysis closer to pinpointing three relevant events. In order to make real headwinds, we cannot divorce our statistics from group level assignment.

A different high level view of relevant phrases is provided by the below graph:

```{r}

np_igraph <- np_clean%>%
  dplyr::select(doc_id, clean_text)%>%
  igraph::graph_from_data_frame()
  
V(np_igraph)$type <- bipartite_mapping(np_igraph)$type
  
np_igraph %>%  
  ggraph(layout = "nicely")+
  geom_edge_link(show.legend = FALSE)+
  geom_node_point(aes(colour = type), size = 5)+
  geom_node_text(aes(label = name),
                 size = 4,
                 vjust = 1.3,
                 hjust = 0.5)+
  theme(legend.position = "none",
        panel.background = element_blank(),
        panel.border = element_rect(colour = "black"))

```

Given the above graphic, you can extract multiple key takeaways about the event landscape you are a part of. High levels of fragmentation would signify an overall lack of synchronicity among events. A more interconnected cobweb of key phrases and events would support the belief that there is a dominant trend.

We can observe some clustering of events allowing us to establish some inter-connectivity if any. We can further refine our uni-gram and noun phrase matching to include only events with relevant words. We utilize lexicons such as the one below as a comparative list.

```{r}
networking_words <- list(
  
  keywords = c("network","networking", "meet", "job", "work","friend",
               "relationship", "mixer","peer"),
  
  others = c("engage", "exchange","social", "mingle", "introduction", 
             "collaborate", "partnership", "discuss", "roundtable")
  
)

```

```{r}
persona_finance_student <- list(
  
  keywords = c(
    "finance", "financial", "investment", "investing", "markets",
    "economics", "trading", "derivatives", "valuation", "analytics",
    "data", "economy", "portfolio", "risk", "venture", "credit", "debt",
    "banking", "capital", "wealth", "equity", "fund", "money"
  ),
  
  priority_keywords = c("finance", "internship", "trading", "analyst", "mentorship", "professional"),
  
  locations = c("virtual", "online"),
  
  event_types = c("networking", "career fair", "seminar")
)


data.frame(Keywords = paste(persona_finance_student$keywords, collapse = ", "), Priority_Keys = paste(persona_finance_student$priority_keywords, collapse = ", "), Locations = paste(persona_finance_student$locations, collapse = ", "))%>%
  gt::gt() %>% 
  cols_label(Priority_Keys = "Priority Keywords")

```

```{r}
persona_tech_student <- list(
  
  keywords = c(
    "tech", "technology", "software", "developer", "coding",
    "programming", "data", "python", "cloud", "app",
    "machine", "learning", "ai", "intelligence", "artificial"
  ),
  
  priority_keywords = c(
    "hackathon", "developer", "internship", "programming"
  ),
  
  locations = c("calgary"),
  
  event_types = c("workshop", "webinar", "hackathon")
)

```

```{r}
persona_entrepreneur <- list(
  
  keywords = c(
    "entrepreneur", "startup", "business", "innovation",
    "pitch", "founder", "funding", "capital", "growth", "strategy",
    "marketing", "strategy", "leadership", "sales", "money", "sales"
  ),
  
  priority_keywords = c(
    "sales", "pitch", "startup", "entrepreneur", "mentor"
  ),
  
  locations = c("virtual", "online"),
  
  event_types = c(
    "networking", "pitch", "seminar"
  )
)
```

### 3.3.2. Persona and Lexicon Matching

To score events by relevance, we match keywords in pre-defined lexicons containing preferred language, with each event's description. Below is a visual example of the first step in our machine learning workflow.

```{r}


topic <- persona_finance_student$keywords # choose your topic of interest here

nlp_token_1$Contains <- "" # initializing the contains column 


for(i in 1:nrow(nlp_token_1)){

nlp_token_1$Contains[i] <- grepl(nlp_token_1[i,2], paste(topic, collapse = ""))

}

nlp_fit <- nlp_token_1 %>%
  dplyr::filter(Contains == "TRUE")%>%
  dplyr::group_by(event_id)%>%
  dplyr::count(word, name = "Matches")%>%
  dplyr::arrange(desc(Matches))

nlp_fit %>%
  dplyr::group_by(event_id)%>% dplyr::summarise(Matches = sum(Matches))%>%
  dplyr::arrange(desc(Matches))%>% dplyr::slice_head(n = 10)%>%
  ggplot2::ggplot(aes(x = Matches, y = reorder(event_id, Matches), fill = Matches))+
  geom_col()+
  theme(panel.background = element_blank(),
        panel.grid = element_line(colour = "lightgrey"),
        legend.position = "none")+
  labs(x = "Matches", y = "Event ID",
       title = "Number of Persona Matches by Event")




```

At its core, we count matches between defined criteria and event descriptions. In the above example, taking count as score, the top ranking event will thus be recommended to our end user. This is a simplified example.

```{r}

nlp_type <- nlp_token_1
nlp_type$Networking <- ""

for(i in 1:nrow(nlp_type)){

nlp_type$Networking[i] <- grepl(nlp_type[i,2], paste(networking_words, collapse = ""))
}

nlp_type <- nlp_type %>%
  dplyr::mutate(across(3:ncol(nlp_type), ~ as.integer(as.logical(.))))

# Not generalized so far
nlp_type_scores <- nlp_type%>%
  dplyr::group_by(event_id)%>%
  dplyr::summarise(Network_Score = sum(Networking),
                   Matches = sum(Contains))%>%
  dplyr::filter(Matches == 1)

```

*Shown above is another example of key word matching, this time however, we are calculating a sentiment score based on the event's inclination towards the use of networking words.*

### 3.3.3. N-gram Matching

Expanding from the uni-gram analysis, phrases can provide extra relevance, especially if repeated within or across events. Below are network graphs with information we will combine and use to assess relevance for each event.

```{r}


persona1 <- persona_tech_student


  
np_clean$Contains = ""
  
for(i in 1:nrow(np_clean)){  

  for(j in 1:length(persona1$keywords)){
    
      np_clean$Contains[i] <-grepl(persona1$keywords[j], 
                                    np_clean$clean_text[i],
                                    ignore.case = TRUE)
  
      if(np_clean$Contains[i] == TRUE){
        
        break
        
      } else{
        
        next
        
      }
  }
}




np_graph_hold <- np_clean  %>% dplyr::filter(Contains != "FALSE")%>%
  dplyr::select(doc_id, clean_text)%>%
  igraph::graph_from_data_frame()

V(np_graph_hold)$type <- bipartite_mapping(np_graph_hold)$type

np_graph_hold%>%
  ggraph(layout = "fr") +
  geom_edge_link(show.legend = FALSE) +
  geom_node_point(aes(color = type), size = 5) +
  geom_node_text(aes(label = name),
                 size = 3,
                 vjust = 1.5,
                 hjust = .4) +
  theme(panel.background = element_blank(),
        legend.position = "none")+
  labs(title = "Phrases in Relevant Events")




np_holder <- np_clean %>%
  dplyr::filter(Contains != "FALSE")


```

```{r}

np_pairwise <- np_clean %>%
  dplyr::filter(doc_id %in% np_holder$doc_id)%>%
  dplyr::select(doc_id, clean_text)%>%
  widyr::pairwise_cor(
    item = clean_text,
    feature = doc_id,
    sort = T,
    upper = FALSE)%>%
  dplyr::filter(correlation > 0)


np_pairwise%>%
  igraph::graph_from_data_frame() %>%
  ggraph(layout = "grid") +
  geom_edge_link(show.legend = FALSE) +
  geom_node_point(color = "orange", size = 5) +
  geom_node_text(aes(label = name),
                 size = 3,
                 vjust = 1.5,
                 hjust = .5) +
  theme_void()+
  labs(title = "Correlated Phrases in Relevant Events")



```

The First - "Phrases in Relevant Events" - is a network graph tying event_id with all known noun phrases that include relevant information. This will not directly be used to calculate relevant score because keyword matching is already incorporated through uni-gram matching.

Each individual edge node does not reflect significance by virtue of existing. An exception is phrase nodes connecting to 1 or more event. If a noun-phrase containing a key word is mentioned in more than one event, we consider it a "Trendy" word. This can aid our analysis if we can isolate a trendiness score per event.

The Second - "Correlated Phrases in Relevant Events" - is a pairwise correlation between noun-phrases, not grouped by event. While this provides useful information on what words are used together, and which are not on a large scale, we use it to calculate trendiness.

In simple terms, our pairwise output links all different phrases used together across event descriptions. We use this to analyze the number of pairings each event has generated via its noun phrases.

[Defining Trendiness:]{.underline}

Given a count for noun phrases per event, and pair links generated per event, we calculate trendiness as:

(# of Pairs) / ^(# of noun phrases)^C~2~

We calculate trendiness as such because pairwise analysis provides pairings for noun phrases within the same event, potentially leading to incorrect interpretations.

If the numerator (# of Pairs) is larger than the possible combinations of noun phrase pairs per event, we can conclude that one or more noun phrases in that event is paired with a different noun phrase else where.

An output of the scoring is found below:

```{r}

# Trendiness Score

np_clean_ids <- np_clean %>%
  dplyr::filter(doc_id %in% np_holder$doc_id)

# ^ Grabs all noun phrases for relevant events

noun_count <- np_clean_ids%>%
  dplyr::group_by(doc_id)%>%
  count(doc_id,
        name = "count_nouns")

np_pairs <- np_pairwise %>% count(item1, name = "Pairs")


np_pairs <- np_pairs %>%
  left_join(np_clean_ids %>% select(clean_text, doc_id),
          by = c("item1" = "clean_text"))



np_pairs$doc_id <- as.integer(np_pairs$doc_id)

np_pairs %>%
  dplyr::select(-item1)%>%
  dplyr::group_by(doc_id)%>%
  dplyr::summarise(Pairs = sum(Pairs))%>%
  dplyr::left_join(noun_count)%>%
  dplyr::mutate(Trendiness = Pairs/
                  pmax(choose(count_nouns, 2), 1))%>%
  drop_na()%>%
  ggplot2::ggplot(aes(x = reorder(as.character(doc_id), Trendiness),
                      y = Trendiness,
                      fill = as.character(doc_id)))+
  geom_col()+
  labs(title = "Trendiness Meter Per Event",
       x = "Event IDs")+
  theme(panel.background = element_blank(),
        legend.position = "none")




```

*In a highly idiosyncratic event landscape, you can expect a default trendiness score of 1 across the board.*

## 3.4. Machine Learning Workflow

```{r, include = FALSE}
# attempt at making ML workflow chart

workflow_graph <- data.frame(Workflow = c("Criteria Matching",
                                       "Building Score Tables",
                                       "Calculating Relevant Score",
                                       "Identifying Top Events",
                                       "Collecting Information"),
                       Order = c(1:5),
                       y_ax = c(2,2,2,2,2))

workflow_graph %>%
  ggplot2::ggplot(aes(x = Order, 
                      y = y_ax, fill = Workflow,
                      label = Workflow))+
  geom_point(size = 3,
             colour = "black")+
  theme(panel.background = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.line.x = element_blank(),
        axis.line.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        legend.position = "none")
```

We developed a scalable, data-driven event recommendation model designed to evaluate and rank networking opportunities for personas with diverse interests and objectives. The approach integrates multiple structured features, including persona-specific keyword relevance, sentiment indicators, location and event-type alignment, and trend analysis. Each feature is weighted according to its relative importance within the scoring framework. Using this structure, the model assigns every event a standardized relevance score ranging from 1 to 10. This enables a clear analysis based comparison of opportunities and highlights the highest-value events for any given persona. The resulting recommendations provide a targeted and systematic method for connecting individuals with the most meaningful and engaging events in the dataset.

Our model follows a simple, yet effective workflow:

-   Keyword Alignment: We assess how strongly an event’s title and description reflect a personas interests by matching relevant domain-specific terms.

-   Sentiment Analysis: Networking sentiment scores evaluate an event's description against a lexicon containing key words and other networking words, with both columns weighted differently.

<!-- -->

-   Location & Event-Type Relevance: Events recieve a higher score when their format, setting or location align more closely with persona preference.

<!-- -->

-   Trendiness Analysis: Instances of noun phrase pairings across event descriptions are factored by noun phrase counts per event to evaluate the level at which an event matches others in the space.

-   NER: All special entities are used to identify how complete an event is **(finish soon)** , while noun phrases contribute to pairwise correlation calculations. Both of these factors enhance reliability of the trendiness score.

-   Weighted Relevance Score: Each feature contributes proportionally to the final score, which serves as the models indicator of how strongly an event matches the given persona.

```{r}
desc_tokens <- all_events %>%
  select(event_id, description) %>%
  filter(!is.na(description)) %>%
  tidytext::unnest_tokens(output = word, input = description) %>%
  anti_join(tidytext::stop_words)

title_tokens <- all_events %>%
  select(event_id, title) %>%
  filter(!is.na(title)) %>%
  tidytext::unnest_tokens(output = word, input = title) %>%
  anti_join(tidytext::stop_words, by = "word")

```

```{r}
# DONE MATCH DESC SYNC WITH NLP
match_desc <- function(tokens, persona) {

  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }
# counting each instance per event that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Desc_Matches")
  
}

#__________________________________________________________________________________
# DONE MATCH TITLE SYNC WITH NLP
match_title <- function(tokens, persona) {
  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }

# counting each instance per event title that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Title_Matches")
}

#__________________________________________________________________________________

event_sentiment_key <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$keywords, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Key_Matches")


}

#__________________________________________________________________________________

event_sentiment_other <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$others, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Other_Matches")

}

#__________________________________________________________________________________


match_location <- function(events, persona) {
  events %>%
    mutate(
      location_lower = tolower(location),
      Location_Matches = if_else(
        str_detect(location_lower, str_c(persona$locations, collapse = "|")),
        1, 0, missing = 0)) %>%
    select(event_id, Location_Matches)
}


match_type <- function(events, persona) {
  events %>%
    mutate(
      desc_lower = tolower(description),
      Type_Matches = if_else(
        str_detect(desc_lower, str_c(persona$event_types, collapse = "|")),
        1L, 0L, missing = 0L)) %>%
    select(event_id, Type_Matches)
}

#__________________________________________________________________________________

match_trendy <- function(np_clean){ 

np_holder <- np_clean %>%
  dplyr::filter(Contains != "FALSE")
  
  
np_pairwise <- np_clean %>%
  dplyr::filter(doc_id %in% np_holder$doc_id)%>%
  dplyr::select(doc_id, clean_text)%>%
  widyr::pairwise_cor(
    item = clean_text,
    feature = doc_id,
    sort = T,
    upper = FALSE)%>%
  dplyr::filter(correlation > 0)

# Complete repeat of the visualization above ^^^
np_clean_ids <- np_clean %>%
  dplyr::filter(doc_id %in% np_holder$doc_id)

# ^ Grabs all noun phrases for relevant events

noun_count <- np_clean_ids%>%
  dplyr::group_by(doc_id)%>%
  count(doc_id,
        name = "count_nouns")

np_pairs <- np_pairwise %>% count(item1, name = "Pairs")


np_pairs <- np_pairs %>%
  left_join(np_clean_ids %>% select(clean_text, doc_id),
          by = c("item1" = "clean_text"))



np_pairs$doc_id <- as.integer(np_pairs$doc_id)

np_pairs %>%
  dplyr::select(-item1)%>%
  dplyr::group_by(doc_id)%>%
  dplyr::summarise(Pairs = sum(Pairs))%>%
  dplyr::left_join(noun_count)%>%
  dplyr::mutate(Trendiness = Pairs/
                  pmax(choose(count_nouns, 2), 1))%>%
  drop_na()%>%
  dplyr::transmute(event_id = doc_id, Trendiness)


}
#__________________________________________________________________________________







match_highly_relevant <- function(){
 
}






```

```{r}
build_event_features <- function(all_events, desc_tokens, title_tokens, 
                                 persona, networking_words, np_clean) {
  
  desc_hits_tbl     <- match_desc(desc_tokens, persona)
  title_hits_tbl    <- match_title(title_tokens, persona)
  sentiment_key_tbl <- event_sentiment_key(desc_tokens, networking_words)
  sentiment_other_tbl <- event_sentiment_other(desc_tokens, networking_words)
  loc_match_tbl     <- match_location(all_events, persona)
  type_match_tbl    <- match_type(all_events, persona)
  trendy_tbl        <- match_trendy(np_clean)
  
  all_events %>%
    left_join(desc_hits_tbl,  by = "event_id") %>%
    left_join(title_hits_tbl, by = "event_id") %>%
    left_join(sentiment_key_tbl, by = "event_id") %>%
    left_join(sentiment_other_tbl, by = "event_id") %>%
    left_join(loc_match_tbl,  by = "event_id") %>%
    left_join(type_match_tbl, by = "event_id") %>%
    left_join(trendy_tbl,     by = "event_id")%>%
    mutate(
      Desc_Matches          = replace_na(Desc_Matches, 0),
      Title_Matches         = replace_na(Title_Matches, 0),
      Sentiment_Key_Matches       = replace_na(Sentiment_Key_Matches, 0),
      Sentiment_Other_Matches       = replace_na(Sentiment_Other_Matches, 0),
      Location_Matches      = replace_na(Location_Matches, 0),
      Type_Matches          = replace_na(Type_Matches, 0),
      Trendiness            = replace_na(Trendiness, 0)
    )
}
```

```{r}
score_event_relevance <- function(events_features) {
  
  scored <- events_features %>%
    mutate(
      raw_score =
        0.3 * Desc_Matches +
        0.15 * Title_Matches +
        0.15 * Sentiment_Key_Matches +
        0.05 * Sentiment_Other_Matches +
        0.1 * Location_Matches +
        0.15 * Type_Matches +
        0.1 * Trendiness
    )
  
  min_val <- min(scored$raw_score, na.rm = TRUE)
  max_val <- max(scored$raw_score, na.rm = TRUE)
  
  scored %>%
    mutate(relevance_score = round(1 + 9 * ((raw_score - min_val) / (max_val - min_val)), 2)) %>%
    
    arrange(desc(relevance_score))
}

```

```{r}
get_top_events <- function(events_scored, n = 3) {
  events_scored %>%
    slice_head(n = n) %>%
    select(
      title,
      date,
      start_time,
      end_time,
      location,
      description,
      event_url,
      relevance_score
    )
}
```

# 4. Results

From our process to determine relevance score, we have the following results:

-   Finance persona:

```{r}
events_features_1 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_finance_student,
  networking_words,
  np_clean
)

events_scored_1 <- score_event_relevance(events_features_1)
top3_events_1 <- get_top_events(events_scored_1, 3)

top3_events_1 %>%
  gt::gt()%>%
  cols_align("center", columns = c(2:5,8))%>%
  cols_width(title ~ px(400),
             date ~ px(100),
             start_time ~ px(400),
             end_time ~ px(100),
             location ~ px(200),
             description ~ px(500),
             event_url ~ px(400),
             relevance_score ~ px(150))%>%   
  cols_move_to_start(relevance_score)%>%   
  cols_label(relevance_score = "Relevance Score",
             title = "Title",
             date = "Date",
             start_time = "Start Time",
             end_time = "End Time",
             location = "Location",
             description = "Description",
             event_url = "URL")%>%   
  tab_style(style = list(
    cell_fill(color = "lightcyan")),
    locations = cells_column_labels(columns = c(1:8)))
```

-   Tech persona:

```{r}
events_features_2 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_tech_student,
  networking_words,
  np_clean
)

events_scored_2 <- score_event_relevance(events_features_2)
top3_events_2 <- get_top_events(events_scored_2, 3)
top3_events_2 %>%
  gt::gt()%>%
  cols_align("center", columns = c(2:5,8))%>%
  cols_width(title ~ px(400),
             date ~ px(100),
             start_time ~ px(400),
             end_time ~ px(100),
             location ~ px(200),
             description ~ px(500),
             event_url ~ px(400),
             relevance_score ~ px(150))%>%   
  cols_move_to_start(relevance_score)%>%   
  cols_label(relevance_score = "Relevance Score",
             title = "Title",
             date = "Date",
             start_time = "Start Time",
             end_time = "End Time",
             location = "Location",
             description = "Description",
             event_url = "URL")%>%   
  tab_style(style = list(
    cell_fill(color = "lightcyan")),
    locations = cells_column_labels(columns = c(1:8)))
```

-   Entrepreneurship persona:

```{r}
events_features_3 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_entrepreneur,
  networking_words,
  np_clean
)

events_scored_3 <- score_event_relevance(events_features_3)
top3_events_3 <- get_top_events(events_scored_3, 3)
top3_events_3 %>%
  gt::gt()%>%
  cols_align("center", columns = c(2:5,8))%>%
  cols_width(title ~ px(400),
             date ~ px(100),
             start_time ~ px(400),
             end_time ~ px(100),
             location ~ px(200),
             description ~ px(500),
             event_url ~ px(400),
             relevance_score ~ px(150))%>%   
  cols_move_to_start(relevance_score)%>%   
  cols_label(relevance_score = "Relevance Score",
             title = "Title",
             date = "Date",
             start_time = "Start Time",
             end_time = "End Time",
             location = "Location",
             description = "Description",
             event_url = "URL")%>%   
  tab_style(style = list(
    cell_fill(color = "lightcyan")),
    locations = cells_column_labels(columns = c(1:8)))
```

# 5. Conclusion

One key benefit of this project are the clear opportunities to extend and further develop the work. The core underlying framework can easily accommodate additional personas, new event platforms, or more advanced features. With a more in-depth analysis and user feedback, the model could evolve into a fully automated and continuously updated recommendation system. It can be deployed as a recurring recommendation tool for career centers, student clubs, or professional organizations, automatically being updated as new events are posted. Overall, this proof of concept illustrates how combining web scraping with NLP-based feature engineering can signifcantly reduce search costs and has the potential to enhance how individuals discover and prioritize high-value networking opportunities, which ultimately enables a more sophisticated and impactful career development.
