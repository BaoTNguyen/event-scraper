---
title: "Event Scraper for In-Person Networking"
author: "Bao Nguyen, Haitam Aksikas, Ayman Arman"
format: html
editor: visual
---

## 

```{r}
library(dplyr) 
library(tidyr) 
library(purrr) 
library(stringr) 
library(tibble) 
library(arrow) 
library(tidyverse) 
library(spacyr) 
library(textdata) 
library(jsonlite) 
library(tidytext)
```

## *Part 1: Scraping the data from 3 event websites (EdmontonRIN, PlatformCalgary, and EventBrite).*

```{r}
edmontonrin <- system("node ./edmontonrin.js", intern = TRUE) %>%
  fromJSON() %>%
  as_tibble()
```

```{r}
platformcalgary <- system("node ./platformcalgary.js", intern = TRUE) %>%
  fromJSON() %>% 
  as_tibble()
```

```{r}
eventbrite <- system("node ./eventbrite.js", intern = TRUE) %>%
  fromJSON() %>% 
  as_tibble()
```

***Putting all the information together in 1 formatted table (all_events)***

```{r}
all_events_raw <- bind_rows(edmontonrin, platformcalgary) %>% 
  dplyr::mutate(event_id = row_number()) %>%
  dplyr::select(event_id, dplyr::everything())

```

```{r}
# Process NER
# Load from feather file (replace with actual tibble later)
# Preprocess description for spaCy
all_events <- all_events_raw %>% 
  mutate(
    description_clean = description %>%
      str_replace_all("([a-z])(\\.[A-Z])", "\\1. \\2") %>%   # add space after a period glued to a capital
      str_replace_all("([a-z])(,[A-Z])", "\\1, \\2") %>%     # add space after a comma glued to a capital
      str_replace_all("([A-Za-z])'s", "\\1 's") %>%          # ensure space before possessive ’s
      str_replace_all("([A-Z]{2,})([A-Z][a-z])", "\\1 \\2") %>%  # split run-on capitals
      str_squish()
  )


# Initialize spaCy
spacy_install()
spacy_download_langmodel("en_core_web_sm")
spacy_initialize(model = "en_core_web_sm")

# Run NER + noun phrases across all descriptions
ner_df <- spacy_extract_entity(
  all_events$description,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% 
  tibble::as_tibble()

# Preprocessing NER dataframe before further cleaning
ner_df_clean <- ner_df %>%
  filter(!str_detect(text, "@"),
         !str_detect(text, "https?://"))

np_df <- spacy_extract_nounphrases(
  all_events$description,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% 
  tibble::as_tibble()


spacy_finalize()

# Keep useful entity types and regroup by category
keep_types <- c("PERSON", "ORG", "GPE")
entity_category_map <- tibble(
  entity_type = keep_types,
  category = c("people", "organizations", "places")
)

# Test entity cleanup (deduplication and normalization)
entities_tbl <- ner_df_clean %>%
  filter(ent_type %in% keep_types) %>%
  mutate(
    entity_id   = row_number(),     # remember each original span
    orig_entity = text              # save original casing
  ) %>%
  unnest_tokens(token, text) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  group_by(doc_id, ent_type, entity_id, orig_entity) %>%
  summarise(
    entity_clean_tokens = list(token),
    .groups = "drop"
  ) %>%
  mutate(
    token_count = lengths(entity_clean_tokens),
    entity_clean = map_chr(entity_clean_tokens, ~ str_c(.x, collapse = " ")),
    entity_clean = str_squish(entity_clean),
    entity_clean = if_else(
      str_detect(orig_entity, "[A-Z]{2,}") | str_detect(orig_entity, "[A-Za-z][A-Z]"),
      orig_entity,                          # keep acronyms/mixed case
      str_to_title(entity_clean)            # normalize regular names
    )
  ) %>%
  filter(entity_clean != "") %>%
  distinct(doc_id, ent_type, entity_clean) %>%
  left_join(entity_category_map, by = c("ent_type" = "entity_type")) %>%
  group_by(doc_id, category) %>%
  summarise(
    entities = list(unique(entity_clean)),   # deduplicated vectors
    .groups = "drop"
  )

entities_tbl_wide <- entities_tbl %>% 
  pivot_wider(names_from = category, values_from = entities)


# Clean up extracted named entities
ner_tokens <- ner_df %>% 
  unnest_tokens(output = word, input = text) %>% 
  anti_join(stop_words, by = "word")

ner_clean <- ner_tokens %>%
  group_by(doc_id, start_id, length) %>%  # keys for an entity
  summarise(clean_text = str_c(word, collapse = " "),
            length = n(),
            .groups = "drop")




# Clean up extracted noun phrases
np_tokens <- np_df %>% 
  unnest_tokens(output = word, input = text) %>% 
  anti_join(stop_words, by = "word")

np_clean <- np_tokens %>%
  group_by(doc_id, root_id, start_id, length) %>%  # keys for a phrase
  summarise(clean_text = str_c(word, collapse = " "),
            length = n(),
            .groups = "drop") %>%
  filter(clean_text != "") %>% 
  filter(length > 1)

nounphrases_tbl <- np_clean %>%
  mutate(clean_text = str_squish(clean_text)) %>%
  group_by(doc_id) %>%
  summarise(noun_phrases = list(unique(clean_text)), .groups = "drop")

# Attach enriched info to combined events for downstream personas / visualizations
all_events_enriched <- all_events %>%
  left_join(entities_tbl_wide,   join_by("event_id" == "doc_id")) %>%
  left_join(nounphrases_tbl, join_by("event_id" == "doc_id"))

all_events_enriched

```

## *Part 2: NLP Analysis (Ayman)*

### *Why are we using NLP?*

-   *Generate a precise understanding of the topic of the event.*

-   *It is possible that the most used words are not the main topic of the event.*

-   *We may find a relevant event, but still may want to understand the central theme.*

-   *We can create criteria by which we can choose top 3 events.*

    -   *We can categorize events (i.e. "networking", "seminar", "conference", etc.)*

    -   *Is it free or paid?*

    -   *Likelihood of professionals attending?*

    -   *Is it local? regional? national?*

    -   *Is the event open to all people/professions?*

    -   *And much much more potentially.*

-   *Understand the sentiment of the event. Is it positive or negative? Emphasis on Opportunity or Danger/Risks?*

### *How can we use NLP?*

-   *We can track how the DoW impacts what sort of event is taking place (12.4.6 Story Through Time)*

-   *Using correlation mapping, we can explore how different themes connect to one another, providing graphs to the end user exploring how their key words are explored in the context of the event.*

-   *We can create a lexicon for words of interest and assign a sentiment score to each event (based on whichever criteria we deem useful)*

------------------------------------------------------------------------

```{r}
library(stringr) 
library(stringi) 
nlp_tokens <- all_events %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description, token = "ngrams", n = 3)

nlp_token_1 <- all_events %>%
  dplyr::select(event_id, description)%>%
  dplyr::filter(!is.na(description))%>%
  tidytext::unnest_tokens(output = word, input = description)%>%
  dplyr::anti_join(tidytext::stop_words)



# We need to remove stop words, I can do so by creating a new column per words, replacing stop words, and seeing what happens



nlp_tokens <- nlp_tokens %>% 
  tidyr::separate_wider_delim(word, delim = " ", names = c("item1",
                                                           "item2",
                                                           "item3"
                                                           ))
  
  

# for loop to remove stop_words
for(i in 2:nrow(nlp_tokens)){
  for(j in 2:length(nlp_tokens)){
    
    if(nlp_tokens[i, j] %in% tidytext::stop_words$word){
      
      nlp_tokens[i,j] <- ""
      
    }else{
      nlp_tokens[i,j] <- nlp_tokens[i,j]
    }
  }
}





nlp_tokens <- nlp_tokens %>% 
  tidyr::unite(item1, 2:length(nlp_tokens),  sep = " ")%>%
  dplyr::transmute(event_id, phrase = item1)

nlp_tokens$phrase <- nlp_tokens$phrase %>% gsub("^\\s+|\\s+$", "", .)

nlp_tokens <- nlp_tokens %>% dplyr::filter(stri_count_words(phrase) >= 3) %>% unique()
```

*Evaluate the entire above chunk for nlp_tokens, which will be the base for further analysis*

### *Lexicon(s) Creation \*\* Requires Updating*

```{r}
networking_words <- list(
  
  keywords = c("network","networking", "meet", "job", "work","friend",
               "relationship", "mixer","peer"),
  
  others = c("engage", "exchange","social", "mingle", "introduction", 
             "collaborate", "partnership", "discuss", "roundtable")
  
)

```

```{r}
persona_finance_student <- list(
  
  keywords = c(
    "finance", "financial", "investment", "investing", "markets",
    "economics", "trading", "derivatives", "valuation", "analytics",
    "data", "economy", "portfolio", "risk", "venture", "credit", "debt",
    "banking", "capital", "wealth", "equity", "fund", "money"
  ),
  
  priority_keywords = c("finance", "internship", "trading", "analyst", "mentorship", "professional"),
  
  locations = c("virtual", "online"),
  
  event_types = c("networking", "career fair", "seminar")
)
```

```{r}
persona_tech_student <- list(
  
  keywords = c(
    "tech", "technology", "software", "developer", "coding",
    "programming", "data", "python", "r", "cloud", "app",
    "machine", "learning", "ai", "intelligence", "artificial"
  ),
  
  priority_keywords = c(
    "hackathon", "developer", "internship", "programming"
  ),
  
  locations = c("calgary"),
  
  event_types = c("workshop", "webinar", "hackathon")
)

```

```{r}
persona_entrepreneur <- list(
  
  keywords = c(
    "entrepreneur", "startup", "business", "innovation",
    "pitch", "founder", "funding", "capital", "growth", "strategy",
    "marketing", "strategy", "leadership", "sales", "money", "sales"
  ),
  
  priority_keywords = c(
    "sales", "pitch", "startup", "entrepreneur", "mentor"
  ),
  
  locations = c("virtual", "online"),
  
  event_types = c(
    "networking", "pitch", "seminar"
  )
)
```


### *NEW IDEAS:*

1.  *I want to know which key words are being used in which events. I can potentially make a faceted graph showing keyword use in descriptions. –\> this lends some weight to understanding the specific topic of each event.*

### [*Outputs:*]{.underline}

### *Bar chart where x axis is count of persona matches and y axis is event id.*

### *nlp_fit, a tibble with event_id, non-stop words, and \# of mentions in description*

```{r}


topic <- persona_finance_student$keywords # choose your topic of interest here

nlp_token_1$Contains <- "" # initializing the contains column 


for(i in 1:nrow(nlp_token_1)){

nlp_token_1$Contains[i] <- grepl(nlp_token_1[i,2], paste(topic, collapse = ""))

}

nlp_fit <- nlp_token_1 %>%
  dplyr::filter(Contains == "FALSE")%>%
  dplyr::group_by(event_id)%>%
  dplyr::count(word, name = "Matches")%>%
  dplyr::arrange(desc(Matches))

nlp_fit %>%
  dplyr::group_by(event_id)%>% dplyr::summarise(Matches = sum(Matches))%>%
  dplyr::arrange(desc(Matches))%>% dplyr::slice_head(n = 10)%>%
  ggplot2::ggplot(aes(x = Matches, y = reorder(event_id, Matches), fill = Matches))+
  geom_col()+
  theme(panel.background = element_blank(), legend.position = "none")+
  labs(x = "Matches", y = "Event ID")




```

```{r}

# Attempting same logic as matches first, **no use for lexicon dataframe
#Initializing dataframe (this is not AI btw)

nlp_type <- nlp_token_1
nlp_type$Networking <- ""



for(i in 1:nrow(nlp_type)){

nlp_type$Networking[i] <- grepl(nlp_type[i,2], paste(networking_words, collapse = ""))


}

nlp_type <- nlp_type %>%
  dplyr::mutate(across(3:ncol(nlp_type), ~ as.integer(as.logical(.))))


nlp_type %>%
  tidyr::pivot_longer(cols = c(4:ncol(nlp_type)),
                      names_to = "Type",
                      values_to = "Score")%>%
  dplyr::filter(Score != 0)%>%
  ggplot2::ggplot(aes(x = as.character(event_id), 
                      y = Score,
                      fill = Score))+
  geom_col()+
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.background = element_rect(fill = "lightgrey"),
        legend.position = "none")+
  labs(title = "Event Type Scores",
       )+
  coord_flip()
  


# Not generalized so far
nlp_type_scores <- nlp_type%>%
  dplyr::group_by(event_id)%>%
  dplyr::summarise(Network_Score = sum(Networking),
                   Matches = sum(Contains))%>%
  dplyr::filter(Matches == 1)

```

```{r}


event_words <- nlp_type %>% dplyr::filter(Contains != 0)%>%
  dplyr::select(event_id)%>% left_join(., nlp_token_1, by = "event_id")%>%dplyr::group_by(event_id)%>%
  dplyr::count(word, name = "Instances")%>%
  dplyr::arrange(desc(Instances))


colors <- rainbow(17, alpha = .5)

colors2 <- apply(event_words[,2:3], 1, FUN = function(x) colors)



event_igraph <- event_words%>%  dplyr::group_by(word)%>%
  igraph::graph_from_data_frame()


event_igraph%>%
  ggraph(layout = "nicely")+
  geom_edge_link(show.legend = FALSE)+
  geom_node_point(colour = "orange", size = 5)+
  geom_node_text(aes(label = name),
                 size = 4,
                 vjust = 1.3,
                 hjust = 0.9)+
  theme_void()


```

### Machine Learning Workflow

```{r}
desc_tokens <- all_events %>%
  select(event_id, description) %>%
  filter(!is.na(description)) %>%
  tidytext::unnest_tokens(output = word, input = description) %>%
  anti_join(tidytext::stop_words)

title_tokens <- all_events %>%
  select(event_id, title) %>%
  filter(!is.na(title)) %>%
  tidytext::unnest_tokens(output = word, input = title) %>%
  anti_join(tidytext::stop_words, by = "word")

```


```{r}
# DONE MATCH DESC SYNC WITH NLP
match_desc <- function(tokens, persona) {

  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }
# counting each instance per event that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Desc_Matches")
  
}

#__________________________________________________________________________________
# DONE MATCH TITLE SYNC WITH NLP
match_title <- function(tokens, persona) {
  
tokens$Contains <- "" # initializing the contains column 

  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(persona, collapse = ""))
  
  }

# counting each instance per event title that a persona word matches 
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Title_Matches")
}

#__________________________________________________________________________________

event_sentiment_key <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$keywords, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Key_Matches")


}

#__________________________________________________________________________________

event_sentiment_other <- function(tokens, lexicon) {

tokens$Contains <- ""

  
  for(i in 1:nrow(tokens)){
  
  tokens$Contains[i] <- grepl(tokens[i,2], paste(lexicon$others, 
                                                       collapse = ""))
  
  }
  
tokens %>%
  filter(Contains != "FALSE") %>%
  select(event_id)%>%
  count(event_id, name = "Sentiment_Other_Matches")

}

#__________________________________________________________________________________


match_location <- function(events, persona) {
  events %>%
    mutate(
      location_lower = tolower(location),
      Location_Matches = if_else(
        str_detect(location_lower, str_c(persona$locations, collapse = "|")),
        1, 0, missing = 0)) %>%
    select(event_id, Location_Matches)
}


match_type <- function(events, persona) {
  events %>%
    mutate(
      desc_lower = tolower(description),
      Type_Matches = if_else(
        str_detect(desc_lower, str_c(persona$event_types, collapse = "|")),
        1L, 0L, missing = 0L)) %>%
    select(event_id, Type_Matches)
}

#__________________________________________________________________________________

match_highly_relevant <- function(){
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
}






```

```{r}
build_event_features <- function(all_events, desc_tokens, title_tokens, 
                                 persona, networking_words) {
  
  desc_hits_tbl     <- match_desc(desc_tokens, persona)
  title_hits_tbl    <- match_title(title_tokens, persona)
  sentiment_key_tbl <- event_sentiment_key(desc_tokens, networking_words)
  sentiment_other_tbl <- event_sentiment_other(desc_tokens, networking_words)
  loc_match_tbl     <- match_location(all_events, persona)
  type_match_tbl    <- match_type(all_events, persona)
  
  all_events %>%
    left_join(desc_hits_tbl,     by = "event_id") %>%
    left_join(title_hits_tbl, by = "event_id") %>%
    left_join(sentiment_key_tbl,    by = "event_id") %>%
    left_join(sentiment_other_tbl,     by = "event_id") %>%
    left_join(loc_match_tbl,     by = "event_id") %>%
    left_join(type_match_tbl,    by = "event_id") %>%
    mutate(
      Desc_Matches          = replace_na(Desc_Matches, 0),
      Title_Matches         = replace_na(Title_Matches, 0),
      Sentiment_Key_Matches       = replace_na(Sentiment_Key_Matches, 0),
      Sentiment_Other_Matches       = replace_na(Sentiment_Other_Matches, 0),
      Location_Matches      = replace_na(Location_Matches, 0),
      Type_Matches          = replace_na(Type_Matches, 0)
    )
}
```

```{r}
score_event_relevance <- function(events_features) {
  
  scored <- events_features %>%
    mutate(
      raw_score =
        1.0 * Desc_Matches +
        1.5 * Title_Matches +
        1.5 * Sentiment_Key_Matches +
        0.2 * Sentiment_Other_Matches +
        1.0 * Location_Matches +
        1.0 * Type_Matches
    )
  
  min_val <- min(scored$raw_score, na.rm = TRUE)
  max_val <- max(scored$raw_score, na.rm = TRUE)
  
  scored %>%
    mutate(relevance_score = round(1 + 9 * ((raw_score - min_val) / (max_val - min_val)), 2)) %>%
    
    arrange(desc(relevance_score))
}

```

```{r}
get_top_events <- function(events_scored, n = 3) {
  events_scored %>%
    slice_head(n = n) %>%
    select(
      title,
      date,
      start_time,
      end_time,
      location,
      description,
      event_url,
      relevance_score
    )
}
```


```{r}
events_features_1 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_finance_student,
  networking_words
)

events_scored_1 <- score_event_relevance(events_features_1)
top3_events_1 <- get_top_events(events_scored_1, 3)
top3_events_1

```



```{r}
events_features_2 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_tech_student,
  networking_words
)

events_scored_2 <- score_event_relevance(events_features_2)
top3_events_2 <- get_top_events(events_scored_2, 3)
top3_events_2
```



```{r}
events_features_3 <- build_event_features(
  all_events,
  desc_tokens,
  title_tokens,
  persona_entrepreneur,
  networking_words
)

events_scored_3 <- score_event_relevance(events_features_3)
top3_events_3 <- get_top_events(events_scored_3, 3)
top3_events_3
```



What Next: 

Rough Structure of Report. Section where each visualization will belong.



Report:

- Intro: Framing the Problem/hypothesis
  - Problems with the job search, and proposal 


































