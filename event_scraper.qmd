---
title: "Event Scraper Ayman"
format: 
  html:
    embed-resources: true
editor: visual
---

## Event Scraper

Project structure:

-   Web scraping (JavaScript): 

1.  For each analyzed website, search for events using these filters

2.  For each event, extract elements for event title, location, time and description as applicable

3.  Have one .js file per website extracted

-   NLP:

1.  Define a list of locations, keywords and topics to search for (can test 2 personas/sets of characteristics to show effective workflow outputs)

2.  Using web scraping outputs, filter based on above characteristics

3.  Populate a table in R with extracted elements in different columns for each event

4.  Tokenize event descriptions

5.  Remove stop words (the, and, is, etc…) to retain relevant words and named entities

6.  Extract information about event dates, locations, and relevant companies/attendees/speakers

7.  Develop a metric for relevance using sentiment analysis based on a combination of word matching frequency with filters, strength of words used, and other factors

8.  Assess all events using this metric

9.  Format and display details of the 3-5 most relevant events

Notes: once workflow is successful, subsequent report will be made for a specific workflow run (Ex. Run on Dec 1, 2025 for 1 week-1 month ahead of data, then do analysis based on this)

## Run Platform Calgary scraper (Node)

```{r}
# Platform Calgary scraper
node_bin <- Sys.which("node")
if (node_bin == "") {
  stop("Node is not available on this system; install it to run the scraper.")
}

script_path <- file.path("platformcalgary.js")
cmd <- sprintf('"%s" "%s"', node_bin, script_path)
scrape_raw <- system(cmd, intern = TRUE)

# Parse JSON output from the scraper into an R data frame
pc_events <- jsonlite::fromJSON(paste(scrape_raw, collapse = "\n"))
print(pc_events)
```

```{r}
# Eventbrite scraper
node_bin <- Sys.which("node")
if (node_bin == "") {
  stop("Node is not available on this system; install it to run the scraper.")
}

# Capture stdout only (discard stderr so logs/errors don't corrupt JSON)
script_path <- file.path("eventbrite.js")
eb_out <- system2(
  node_bin,
  args = sprintf('"%s"', script_path),
  stdout = TRUE,
  stderr = FALSE,
  wait = TRUE
)

# Convert to UTF-8, drop literal newlines, and strip remaining control characters before parsing
eb_raw <- paste(eb_out, collapse = "\n")
eb_utf8 <- iconv(eb_raw, to = "UTF-8", sub = " ")
eb_utf8 <- gsub("\r", "", eb_utf8, fixed = TRUE)
eb_utf8 <- gsub("\n", "", eb_utf8, fixed = TRUE)
bad_codes <- c(0:8, 11, 12, 14:31, 127)
bad_chars <- paste(vapply(bad_codes, intToUtf8, character(1)), collapse = "")
replacement <- strrep(" ", nchar(bad_chars))
eb_clean <- chartr(bad_chars, replacement, eb_utf8)


# Parse JSON output from the scraper into an R data frame
eb_events <- jsonlite::fromJSON(eb_clean)
print(eb_events)
```

```{r}
# ERIN scraper
node_bin <- Sys.which("node")
if (node_bin == "") {
  stop("Node is not available on this system; install it to run the scraper.")
}

script_path <- file.path("edmontonrin.js")
cmd <- sprintf('"%s" "%s"', node_bin, script_path)
scrape_raw <- system(cmd, intern = TRUE)

# Parse JSON output from the scraper into an R data frame
erin_events <- jsonlite::fromJSON(paste(scrape_raw, collapse = "\n"))
print(erin_events)


```

```{r}
# Combine scraper outputs
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)
library(tibble)
library(arrow)

# Ensure we have tibbles (handles NULL if a scraper failed)
scraper_tables <- list(
  platformcalgary = if (exists("pc_events")) as_tibble(pc_events) else tibble(),
  eventbrite = if (exists("eb_events")) as_tibble(eb_events) else tibble(),
  edmontonrin = if (exists("erin_events")) as_tibble(erin_events) else tibble()
)

# Add a platform column for provenance and bind all
combined_events <- scraper_tables %>%
  imap(function(tbl, name) {
    if (nrow(tbl) == 0) return(tbl)
    tbl %>% mutate(platform = name)
  }) %>%
  bind_rows()

print(combined_events)

# Write file to feather for development/testing purposes first
arrow::write_feather(combined_events, "combined_events.feather")

test_feather <- arrow::read_feather("combined_events.feather")
```

## Event Scraper

Project structure:

-   Web scraping (JavaScript): 

1.  For each analyzed website, search for events using these filters

2.  For each event, extract elements for event title, location, time and description as applicable

3.  Have one .js file per website extracted

-   NLP:

1.  Define a list of locations, keywords and topics to search for (can test 2 personas/sets of characteristics to show effective workflow outputs)

2.  Using web scraping outputs, filter based on above characteristics

3.  Populate a table in R with extracted elements in different columns for each event

4.  Tokenize event descriptions

5.  Remove stop words (the, and, is, etc…) to retain relevant words and named entities

6.  Extract information about event dates, locations, and relevant companies/attendees/speakers

7.  Develop a metric for relevance using sentiment analysis based on a combination of word matching frequency with filters, strength of words used, and other factors

8.  Assess all events using this metric

9.  Format and display details of the 3-5 most relevant events

Notes: once workflow is successful, subsequent report will be made for a specific workflow run (Ex. Run on Dec 1, 2025 for 1 week-1 month ahead of data, then do analysis based on this)

## Table Format

Want to have across all scrapers:

-   platform (str): page the event came from

-   title (str): event title

-   event_url (str): link to the event

-   date (str): the date of the event (mm/dd/yyyy)

-   day_of_week (str): what day it is during the week (Monday, Tuesday, etc...)

-   start_time (str): the time the event starts (ex. 11:00 am given 11:00 am - 1:00 pm)

-   end_time (str): the time the event ends (ex. 1:00 pm given 11:00 am - 1:00 pm)

-   location (str): the event location

-   description (str): the full event description
