---
title: "Event Scraper Project"
format: 
  html:
    embed-resources: true
editor: visual
---

## Event Scraper

Project structure:

-   Web scraping (JavaScript): 

1.  For each analyzed website, search for events using these filters

2.  For each event, extract elements for event title, location, time and description as applicable

3.  Have one .js file per website extracted

-   NLP:

1.  Define a list of locations, keywords and topics to search for (can test 2 personas/sets of characteristics to show effective workflow outputs)

2.  Using web scraping outputs, filter based on above characteristics

3.  Populate a table in R with extracted elements in different columns for each event

4.  Tokenize event descriptions

5.  Remove stop words (the, and, is, etc…) to retain relevant words and named entities

6.  Extract information about event dates, locations, and relevant companies/attendees/speakers

7.  Develop a metric for relevance using sentiment analysis based on a combination of word matching frequency with filters, strength of words used, and other factors

8.  Assess all events using this metric

9.  Format and display details of the 3-5 most relevant events

Notes: once workflow is successful, subsequent report will be made in a generalizable manner so that commentary on results will always be relevant (substitute variable names into analysis as placeholders for statistics and other info retrieved)

## Table Format

Want to have across all scrapers:

-   platform (str): page the event came from

-   title (str): event title

-   event_url (str): link to the event

-   date (str): the date of the event (mm/dd/yyyy)

-   day_of_week (str): what day it is during the week (Monday, Tuesday, etc...)

-   start_time (str): the time the event starts (ex. 11:00 am given 11:00 am - 1:00 pm)

-   end_time (str): the time the event ends (ex. 1:00 pm given 11:00 am - 1:00 pm)

-   location (str): the event location

-   description (str): the full event description

```{r, include=FALSE}
# Load libraries
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)
library(tibble)
library(arrow)
library(tidyverse)
library(spacyr)
library(textdata)
library(tidytext)
library(jsonlite)
```

## Run Platform Calgary scraper (Node)

```{r}
# Platform Calgary scraper
pc_events <- system("node ./platformcalgary.js", intern = TRUE) %>% 
  jsonlite::fromJSON() %>%
  dplyr::as_tibble()
```

```{r}
# Eventbrite scraper
eb_events <- system("node ./eventbrite.js", intern = TRUE) %>% 
  jsonlite::fromJSON() %>%
  dplyr::as_tibble()
```

```{r, eval=FALSE}
# ONLY THIS BLOCK WORKS FOR BAO
# Eventbrite scraper
node_bin <- Sys.which("node")
if (node_bin == "") {
  stop("Node is not available on this system; install it to run the scraper.")
}

# Capture stdout only (discard stderr so logs/errors don't corrupt JSON)
script_path <- file.path("eventbrite.js")
eb_out <- system2(
  node_bin,
  args = sprintf('"%s"', script_path),
  stdout = TRUE,
  stderr = FALSE,
  wait = TRUE
)

# Convert to UTF-8, drop literal newlines, and strip remaining control characters before parsing
eb_raw <- paste(eb_out, collapse = "\n")
eb_utf8 <- iconv(eb_raw, to = "UTF-8", sub = " ")
eb_utf8 <- gsub("\r", "", eb_utf8, fixed = TRUE)
eb_utf8 <- gsub("\n", "", eb_utf8, fixed = TRUE)
bad_codes <- c(0:8, 11, 12, 14:31, 127)
bad_chars <- paste(vapply(bad_codes, intToUtf8, character(1)), collapse = "")
replacement <- strrep(" ", nchar(bad_chars))
eb_clean <- chartr(bad_chars, replacement, eb_utf8)


# Parse JSON output from the scraper into an R data frame
eb_events <- jsonlite::fromJSON(eb_clean) %>% 
  dplyr::as_tibble() 
print(eb_events)
```

```{r}
# ERIN scraper
etest_feather <- system("node ./edmontonrin.js", intern = TRUE) %>% 
  jsonlite::fromJSON() %>%
  dplyr::as_tibble()

```

```{r}
# Make a list of tibbles
scraper_tables <- list(pc_events, eb_events, etest_feather)

# Create a combined tibble from the list
combined_events <- dplyr::bind_rows(scraper_tables) %>% 
  dplyr::mutate(event_id = row_number()) %>%
  dplyr::select(event_id, dplyr::everything())

# Write file to feather for development/testing purposes first
arrow::write_feather(combined_events, "combined_events.feather")

test_feather <- arrow::read_feather("combined_events.feather")
```

```{r}
# Tokenize unigrams and remove stopwords (replace test_feather with actual run)
events_tokens <- test_feather %>%
  select(event_id, description) %>%
  filter(!is.na(description)) %>%
  unnest_tokens(output = word, input = description) %>%
  anti_join(stop_words)

events_tokens

```

```{r}
title_tokens <- test_feather %>%
  select(event_id, title) %>%
  filter(!is.na(title)) %>%
  unnest_tokens(output = word, input = title) %>%
  anti_join(stop_words, by = "word")

title_tokens
```

```{r}
# Visualize most used unigrams for all events
events_tokens %>%
  count(word, sort = TRUE) %>%
  top_n(30) %>%
  ggplot(aes(x = reorder(word, n), y = n, fill = word)) +
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  theme(legend.position = "none") +
  ggtitle("Most Used Words In All Event Descriptions")
```


```{r}
# Defining first persona (finance focused)
persona_keywords1 <- c(
  "business", "entrepreneur", "entrepreneurship",
  "networking", "network", "connect",
  
  "finance", "financial", "funding", "grant", "investment", "investor",
  "loan", "capital", "venture", "sales", "money", "accounting", "debt", "credit",
  
  "workshop", "training", "skills", "program", "bootcamp",
  "seminar", "mentor", "mentorship", "coaching", "professional",
  
  "innovation", "strategy", "growth", "scaling"
)

priority_keywords <- c(
  "financial", "finance", "investment", "investor", "capital", "venture"
)

persona_locations <- c("online")
persona_event_types <- c("workshop", "seminar", "networking")
```



```{r}
# Defining second persona (tech focused)
persona_keywords <- c(
  "ai", "artificial", "intelligence", "machine", "learning",
  "ml", "neural", "model", "deep", "algorithm", "automation",
  
  "data", "analytics", "analysis", "data science", "dataset",
  "python", "r", "sql", "cloud",
  
  "developer", "engineering", "software", "coding", "code",
  "programming", "tech", "technology",

  "innovation", "startup", "product", "scaling", "tech"
)

priority_keywords <- c(
  "ai", "machine", "learning", "ml", "deep", "neural",
  "algorithm", "automation", "robotics",
  "python", "cloud", "technology",
  "data", "developer", "software"
)

persona_locations <- c("online", "virtual", "remote")

persona_event_types <- c(
  "workshop", "hackathon", "seminar", "bootcamp"
)
```



```{r}
# Calculate description unigram matches
desc_hits_tbl <- events_tokens %>%
  filter(word %in% persona_keywords) %>%
  count(event_id, name = "desc_hits")

# Calculate priority unigram matches
priority_hits_tbl <- events_tokens %>%
  filter(word %in% priority_keywords) %>%
  count(event_id, name = "priority_hits")

# Calculate title unigram matches
title_hits_tbl <- title_tokens %>%
  filter(word %in% persona_keywords) %>%
  count(event_id, name = "title_hits")

# Sentiment analysis on each cleaned token
afinn <- tidytext::get_sentiments("afinn")
event_sentiment <- events_tokens %>%
  inner_join(afinn, by = "word") %>%
  group_by(event_id) %>%
  summarise(sentiment = mean(value), .groups = "drop")

# Calculate location unigram matches
loc_match_tbl <- test_feather %>%
  mutate(
    location_lower = tolower(location),
    loc_match = if_else(
      str_detect(location_lower, str_c(persona_locations, collapse = "|")), 
      1L, 0L, missing = 0L)) %>%
  select(event_id, loc_match)

# Calculate event type unigram matches
type_match_tbl <- test_feather %>%
  mutate(
    desc_lower = tolower(description),
    type_match = if_else(
      str_detect(desc_lower, str_c(persona_event_types, collapse = "|")),
      1L, 0L, missing = 0L
    )) %>%
  select(event_id, type_match)
```



```{r}
# Put together all matches
events_features <- test_feather %>%
  left_join(desc_hits_tbl, by = "event_id") %>%
  left_join(priority_hits_tbl, by = "event_id") %>%
  left_join(title_hits_tbl, by = "event_id") %>%
  left_join(event_sentiment, by = "event_id") %>%
  left_join(loc_match_tbl, by = "event_id") %>%
  left_join(type_match_tbl, by = "event_id") %>%
  
  mutate(
    desc_hits = replace_na(desc_hits, 0L),
    priority_hits = replace_na(priority_hits, 0L),
    title_hits = replace_na(title_hits, 0L),
    sentiment = replace_na(sentiment, 0),
    loc_match = replace_na(loc_match, 0L),
    type_match = replace_na(type_match, 0L)
  )

events_features
```



```{r}
# Calculate relevance score based on formula
events_scored <- events_features %>%
  mutate(
    relevance_score =
      1.0 * desc_hits +
      2.0 * title_hits +
      1.5 * priority_hits +
      0.2 * sentiment +
      1.0 * loc_match +
      1.0 * type_match
  ) %>%
  arrange(desc(relevance_score))

events_scored
```



```{r}
# Retrieve top 3 relevant events
top3_events <- events_scored %>%
  slice_head(n = 3)

# Display event info conveniently
top3_events %>%
  select(
    title,
    date,
    start_time,
    end_time,
    location,
    description,
    event_url
  )
```



```{r}
# Process NER
# Load from feather file (replace with actual tibble later)
# Preprocess description for spaCy
combined_events <- read_feather("combined_events.feather") %>%
  mutate(
    description_clean = description %>%
      str_replace_all("([a-z])(\\.[A-Z])", "\\1. \\2") %>%   # add space after a period glued to a capital
      str_replace_all("([a-z])(,[A-Z])", "\\1, \\2") %>%     # add space after a comma glued to a capital
      str_replace_all("([A-Za-z])'s", "\\1 's") %>%          # ensure space before possessive ’s
      str_replace_all("([A-Z]{2,})([A-Z][a-z])", "\\1 \\2") %>%  # split run-on capitals
      str_squish()
  )


# Initialize spaCy
spacy_install()
spacy_download_langmodel("en_core_web_trf")
spacy_initialize(model = "en_core_web_trf")

# Run NER + noun phrases across all descriptions
ner_df <- spacy_extract_entity(
  combined_events$description,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% 
  tibble::as_tibble()

# Preprocessing NER dataframe before further cleaning
ner_df_clean <- ner_df %>%
  filter(!str_detect(text, "@"),
         !str_detect(text, "https?://"))

np_df <- spacy_extract_nounphrases(
  combined_events$description,
  output = "data.frame"
) %>% 
  mutate(doc_id = as.integer(str_remove(doc_id, "^text"))) %>% 
  tibble::as_tibble()


spacy_finalize()

# Keep useful entity types and regroup by category
keep_types <- c("PERSON", "ORG", "GPE")
entity_category_map <- tibble(
  entity_type = keep_types,
  category = c("people", "organizations", "places")
)

# Test entity cleanup (deduplication and normalization)
entities_tbl <- ner_df_clean %>%
  filter(ent_type %in% keep_types) %>%
  mutate(
    entity_id   = row_number(),     # remember each original span
    orig_entity = text              # save original casing
  ) %>%
  unnest_tokens(token, text) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  group_by(doc_id, ent_type, entity_id, orig_entity) %>%
  summarise(
    entity_clean_tokens = list(token),
    .groups = "drop"
  ) %>%
  mutate(
    token_count = lengths(entity_clean_tokens),
    entity_clean = map_chr(entity_clean_tokens, ~ str_c(.x, collapse = " ")),
    entity_clean = str_squish(entity_clean),
    entity_clean = if_else(
      str_detect(orig_entity, "[A-Z]{2,}") | str_detect(orig_entity, "[A-Za-z][A-Z]"),
      orig_entity,                          # keep acronyms/mixed case
      str_to_title(entity_clean)            # normalize regular names
    )
  ) %>%
  filter(entity_clean != "") %>%
  distinct(doc_id, ent_type, entity_clean) %>%
  left_join(entity_category_map, by = c("ent_type" = "entity_type")) %>%
  group_by(doc_id, category) %>%
  summarise(
    entities = list(unique(entity_clean)),   # deduplicated vectors
    .groups = "drop"
  )

entities_tbl_long <- entities_tbl %>% 
  pivot_wider(names_from = category, values_from = entities)


# Clean up extracted named entities
ner_tokens <- ner_df %>% 
  unnest_tokens(output = word, input = text) %>% 
  anti_join(stop_words, by = "word")

ner_clean <- ner_tokens %>%
  group_by(doc_id, start_id, length) %>%  # keys for an entity
  summarise(clean_text = str_c(word, collapse = " "),
            length = n(),
            .groups = "drop")




# Clean up extracted noun phrases
np_tokens <- np_df %>% 
  unnest_tokens(output = word, input = text) %>% 
  anti_join(stop_words, by = "word")

np_clean <- np_tokens %>%
  group_by(doc_id, root_id, start_id, length) %>%  # keys for a phrase
  summarise(clean_text = str_c(word, collapse = " "),
            length = n(),
            .groups = "drop") %>%
  filter(clean_text != "") %>% 
  filter(length > 1)

nounphrases_tbl <- np_clean %>%
  mutate(clean_text = str_squish(clean_text)) %>%
  group_by(doc_id) %>%
  summarise(noun_phrases = list(unique(clean_text)), .groups = "drop")

# Attach enriched info to combined events for downstream personas / visualizations
combined_events_enriched <- combined_events %>%
  left_join(entities_tbl_long,   join_by("event_id" == "doc_id")) %>%
  left_join(nounphrases_tbl, join_by("event_id" == "doc_id"))

combined_events_enriched

```



```{r}
# Perform NER on most relevant events (irrelevant now)
# spacy_install()
# spacy_initialize(model = "en_core_web_trf")
# 
# ner_people <- spacy_extract_entity(top3_events$description,
#   output = "list",
#   type = "named"
# )
# 
# spacy_finalize()
# 
# print(ner_people)
```
